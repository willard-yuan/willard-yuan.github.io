<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="aexITLS38FdIRzwj25OVWxm87rpa9l-UV0URTyC9cTs" />
  <title>
    机器学习：一步步教你理解反向传播方法
    
  </title>
  <link rel="stylesheet" href="../css/site.css">
  <link href="//cdn.bootcss.com/font-awesome/4.4.0/css/font-awesome.css" rel="stylesheet">
  <link href="/favicon.ico" rel="shortcut icon" type="image/x-icon" />
  
  <link href="/blog/feed.xml" type="application/rss+xml" rel="alternate" title="Latest 10 blog posts (atom)" />

  

  

  
</head>

<body>
  <div class="header-container">
    <header class="inner">
      
      <nav>
        <a class="" href="/">About</a>
        <a class="" href="/project/">Project</a>
        <!--<a class="" href="/resume/">Resume</a>-->
	      <!--<a href="/yuanyong_resume_cn.pdf">Resume</a>-->
	      <a href="/YongYuan_resume.pdf">Resume</a>
        <!--<a class="" href="/publication/">Publication</a>-->
        <a class="" href="/blog/">Blog</a>
	      <a href="/books/">Book</a>
		    <a href="/timeline/">Times</a>
        <a href="https://github.com/willard-yuan">GitHub</a>
      </nav>
	  

      <div class="pull-right right logo">
        <!--<div class="name">
          <a href="/">Yong Yuan</a><br />
          <small>
            <em>
              
                <a href="/">developer</a>
              
            </em>
          </small>
        </div>-->
        <a href="http://yongyuan.name/"><img class="avatar" src="http://ose5hybez.bkt.clouddn.com/index/YongYuan.png" alt="My profile picture" /></a>
      </div>
      <div class="clear"></div>
    </header>
    <div class="clear"></div>
  </div>

  <link rel="stylesheet" href="../css/blog.css">



<div id="container">
<article>
<div class="inner">
<div class = "post-header">
  <h1>
    机器学习：一步步教你理解反向传播方法
  </h1>
  <ul class="meta">
  	<i class="fa fa-calendar"></i>
  	2016年09月12日
  	<i class="space"></i>
  	<i class="fa fa-archive"></i>
  	<a class="tag" href="http://localhost:4000/blog/categories.html#机器学习">机器学习</a>
  	<i class="space"></i>
  	<i class="fa fa-tags"></i>
  	
  	    <a class="tag" href="http://localhost:4000/tag/#机器学习">机器学习</a>
  	    <i class="space"></i>
  	
  	<i class="space"></i> 字数:5596
  </ul>
</div>
</div>

  <div class="post">
        <div id="content">
          <div class ="sidebox">
              <div class ="sidebar">
              <div class="toc"></div>
              </div>
          </div>
  	   <blockquote>
  <p>在阅读反向传播方法的时候，看到了这篇通过示例给出反向传播的博文<a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">A Step by Step Backpropagation Example</a>，在这篇博文中，作者通过一个简单的示例，给出了反向传播的过程的过程，非常的清晰，遂结合自己的理解翻译之，希望对反向传播方法有所理解的朋友有些许帮助。</p>
</blockquote>

<h2 id="背景">背景</h2>

<p>反向传播在神经网络的训练过程中虽然用得如此之多，但是在网上还很少有通过具体的实例来解释反向传播怎么工作的博文。所以在这篇文章中，我会尝试用一个具体的例子来解释反向传播过程，这样有需要的朋友就可以通过自己的计算过程来判断自己对于反向理解的过程是否到位。</p>

<p>你可以在我的Gihub上找个我写的反向传播的Python实现代码。</p>

<h2 id="概览">概览</h2>

<p>在这篇博文中，我们会使用有2个输入单元的神经网络，2个隐层神经元以及2个输出神经元。此外，隐层和输出神经元会包含一个偏置，下面是基本的网络结构：</p>

<p><img src="https://matthewmazur.files.wordpress.com/2018/03/neural_network-7.png?w=525" alt="" /></p>

<p>为了便于后面说明的说明，我们对该网络设置一些初始的权重、偏置以及输入和输出：</p>

<p><img src="https://matthewmazur.files.wordpress.com/2018/03/neural_network-9.png?w=525" alt="" /></p>

<p>反向传播的目标是对权重进行优化，使得神经网络能够学习到从任意的输入到输出的准确映射。</p>

<p>在这篇博文中，我们仅使用一个简单的训练集，即输入为0.05和0.10，我们希望网络的输出为0.01和0.99(即输入的样本是两个: (0.05, 0.99), (0.10, 0.99))。</p>

<h2 id="前向传播">前向传播</h2>

<p>首先来看看对于给定的初始化权重和偏置，网络对于输入0.05和0.10的输出是啥。我们将输入输进网络中。</p>

<p>我们先计算从全部网络的输入到隐层的每一个神经元，激活函数采用logistic函数，对于从隐层到输出层，我们重复这一过程。</p>

<blockquote>
  <p>全部的网络输入也被称为网络的输入<a href="http://web.cs.swarthmore.edu/~meeden/cs81/s10/BackPropDeriv.pdf">Derivation of Backpropagation</a></p>
</blockquote>

<p>下面是对于$h_1$全部网络输入的输入计算过程：</p>

<script type="math/tex; mode=display">\begin{equation}
   net_{h1}=w_1*i_1+w_2*i_2+b_1*1\\\\
   net_{h1}=0.15*0.05+0.2*0.1+0.35*1=0.3775
\end{equation}</script>

<p>(译者注：类比到CNN网络里，这个过程就是卷积过程，得到特征响应图)</p>

<p>然后我们将其输入到激活函数中，得到输出$h_1$:</p>

<script type="math/tex; mode=display">\begin{equation}
   out_{h1}=\frac{1}{1+e^{-net_{h1}}}=\frac{1}{1+e^{-0.3775}}=0.593269992
\end{equation}</script>

<p>(译者注：类比到CNN网络里，这个过程特征响应图经过激活函数运算的过程)</p>

<p>对于$h_2$通过上面相同的过程，我们可以得到：</p>

<script type="math/tex; mode=display">\begin{equation}
   out_{h2}=0.596884378
\end{equation}</script>

<p>对于输入层神经元，将隐层的输出作为输入(译者注：在CNN中，还需要经过池化后才能作为下一层的输入，至于为啥需要池化，这里译者不就解释了)，重复上面相同的过程，我们可以得到：</p>

<script type="math/tex; mode=display">\begin{equation}
   net_{o1}=w_5*out_{h1}+w_6*out_{h2}+b_2*1\\\\
   net_{o1}=0.4*0.593269992+0.45*0.596884378+0.6*1=1.105905967\\\\
   out_{o1}=\frac{1}{1+e^{-net_{o1}}}=\frac{1}{1+e^{-1.105905967}}=0.75136507
\end{equation}</script>

<p>同样的，重复上面相同的过程，可以得到$O_2$:</p>

<script type="math/tex; mode=display">\begin{equation}
   out_{O2}=0.772928465
\end{equation}</script>

<h2 id="计算总误差">计算总误差</h2>

<p>现在对于输出的每一个神经元，使用平方误差函数求和来计算总的误差：</p>

<script type="math/tex; mode=display">\begin{equation}
   E_{total}=\sum \frac{1}{2}(target-output)^2
\end{equation}</script>

<blockquote>
  <p>output就是我们的预测label，而target就是groundtruth。$\frac{1}{2}$使得我们在求骗到的时候可以消去2，不影响模型参数的结果求解。</p>
</blockquote>

<p>对于第一个神经元的输出$O1$真实值是0.01，而网络的输出是0.75136507， 因而第一个神经元的输出误差为：</p>

<script type="math/tex; mode=display">\begin{equation}
   E_{O1}=\frac{1}{2}(target-output)^2=\frac{1}{2}(0.01-0.75136507)^2=0.274811083
\end{equation}</script>

<p>重复上面过程，可以得到第二个神经元的输出$O2$为：</p>

<script type="math/tex; mode=display">\begin{equation}
   E_{O2}=0.023560026
\end{equation}</script>

<p>所以整个神经网络的误差求和为：</p>

<script type="math/tex; mode=display">\begin{equation}
   E_{total}=E_{O1}+E_{O2}=0.274811083+0.023560026=0.298371109
\end{equation}</script>

<h2 id="反向传播">反向传播</h2>

<p>反向传播的目标是：通过更新网络中的每一个权重，使得最终的输出接近于groundtruth，这样就得到整个网络的误差作为一个整体进行了最小化。</p>

<h3 id="输出层">输出层</h3>

<p>先来考察$w_5$，我们想知道对于$w_5$的改变可以多大程度上影响总的误差，也就是$\frac{\partial E_{total}}{\partial w_5}$。</p>

<p>通过使用链式法则，可以得到：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial w_5}=\frac{\partial E_{total}}{\partial out_{o1}}*\frac{\partial out_{O1}}{\partial net_{O1}}*\frac{\partial net_{O1}}{\partial w_5}
\end{equation}</script>

<p>为了更直观的表述上面链式法则的过程，对其进行可视化：</p>

<p><img src="https://matthewmazur.files.wordpress.com/2018/03/output_1_backprop-4.png?w=1050" alt="" /></p>

<p>我们对上面使用链式法则得到的每一项分别进行计算。首先，整体误差关于各个神经元的输出改变了？</p>

<script type="math/tex; mode=display">\begin{equation}
   E_{total}=\sum \frac{1}{2}(target-output)^2=\frac{1}{2}(target_{O1}-output_{O1})^2+\frac{1}{2}(target_{O2}-output_{O2})^2 \\\\
   \frac{\partial E_{total}}{\partial out_{O1}}=2*\frac{1}{2}(target_{O1}-output_{O1})^{2-1}*-1+0=-(target_{O1}-output_{O1})=-(0.01-0.75136507)=0.74136507
\end{equation}</script>

<p>logistic函数的偏导数为输出乘以1减去输出，即：</p>

<script type="math/tex; mode=display">\begin{equation}
   out_{O1}=\frac{1}{1+e^{-net_{O1}}}\\\\
   \frac{\partial out_{O1}}{\partial net_{O1}}=out_{O1}(1-out_{O1})=0.75136507(1-0.75136507)=0.186815602
\end{equation}</script>

<p>最后，整个网络的输入$O1$关于$w_5$改变了多少呢？</p>

<script type="math/tex; mode=display">\begin{equation}
   net_{O1}=w_5*out_{h1}+w_6*out_{h2}+b_2*1\\\\
   \frac{\partial E_{total}}{\partial w_5}=\frac{\partial E_{total}}{\partial out_{o1}}*\frac{\partial out_{O1}}{\partial net_{O1}}*\frac{\partial net_{O1}}{\partial w_5}\\\\
   \frac{\partial E_{total}}{\partial w_5}=0.74136507*0.186815602*0.593269992=0.082167041
\end{equation}</script>

<blockquote>
  <p>你也会看到用delta规则表示的形式：</p>

  <script type="math/tex; mode=display">\begin{equation}
    \frac{\partial E_{total}}{\partial w_5} = -(target_{O1}-out_{O1})*out_{O1}(1-out_{o1})*out_{h1}
\end{equation}</script>

  <p>我们可以将$\frac{\partial E_{total}}{\partial out_{O1}}$和$\frac{\partial out_{O1}}{\partial net_{O1}}$写为$\frac{\partial E_{total}}{\partial net_{O1}}$，并用$\delta_{O1}$表示它，从而可以将上面的式子表示为：</p>

  <script type="math/tex; mode=display">\begin{equation}
    \delta_{O1} = \frac{\partial E_{total}}{\partial out_{o1}}*\frac{\partial out_{O1}}{\partial net_{O1}}\\\\
    \delta_{O1} =  -(target_{O1}-out_{O1})*out_{O1}(1-out_{o1})   
\end{equation}</script>

  <p>因此有：</p>

  <script type="math/tex; mode=display">\begin{equation}
    \frac{\partial E_{total}}{\partial w_5}=\delta_{O1}out_{h1}
\end{equation}</script>

  <p>有一些论文中通过将负号从$\delta$中提出来将其也可以写为下面这种形式：</p>

  <script type="math/tex; mode=display">\begin{equation}
    \frac{\partial E_{total}}{\partial w_5}=-\delta_{O1}out_{h1}
\end{equation}</script>
</blockquote>

<p>为了减小误差，我们将$w_{5}$原来的值减去目前的权重(通常会乘上一个学习率$\eta$，这里我们将其设置为0.5)：</p>

<script type="math/tex; mode=display">\begin{equation}
    w^+_5=w_5-\eta*\frac{\partial E_{total}}{\partial w_5}
\end{equation}</script>

<blockquote>
  <p>学习率在不同的文章中可以记法不一样，有用$\alpha$的，有用$\eta$的，有用$\epsilon$的。</p>
</blockquote>

<p>重复上面的过程，我们可以得到更新后的$w_6$、$w_7$和$w_8$：</p>

<script type="math/tex; mode=display">\begin{equation}
    w^+_6=0.408666186
    w^+_7=0.511301270
    w^+_8=0.561370121
\end{equation}</script>

<p>注意，在我们继续向前推进反向传播的时候，在要使用到$w_5$、$w_6$、$w_7$和$w_8$的地方，我们仍然使用的是原来的权重，而不是更新后的权重。</p>

<h2 id="隐层">隐层</h2>

<p>我们继续推进反向传播来计算$w_1$、$w_2$、$w_3$和$w_4$更新的权重：</p>

<p>同样使用链式法则，我们可以得到：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial w_1}=\frac{\partial E_{total}}{\partial out_{h_1}}*\frac{\partial out_{h_1}}{\partial net_{h1}}*\frac{\partial net_{h1}}{\partial w_1}\\\\
\end{equation}</script>

<p>可视化上面的链式法则：</p>

<p><img src="https://matthewmazur.files.wordpress.com/2015/03/nn-calculation.png" alt="" /></p>

<p>对于这一层(隐层)的更新我们采用上面输出层相似的处理方式，不过会稍有不同，这种不同主要是因为每一个隐层神经元的输出对于最终的输出都是有贡献的。我们知道$out_{h1}$既影响$out_{O1}$也影响$out_{O2}$，因此$\frac{\partial E_{total}}{\partial out_{h1}}$需要同时考虑到这两个输出神经元影响：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial out_{h1}}=\frac{\partial E_{O1}}{\partial out_{h_1}}+\frac{\partial E_{O2}}{\partial out_{h1}}
\end{equation}</script>

<p>又由于：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{O1}}{\partial out_{h1}}=\frac{\partial E_{O1}}{\partial net_{O1}}*\frac{\partial net_{O1}}{\partial out_{h1}}
\end{equation}</script>

<p>我们可以用前面计算的值来计算$\frac{\partial E_{O1}}{\partial net_{O1}}$:</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{O1}}{\partial net_{O1}}=\frac{\partial E_{O1}}{\partial out_{O1}}*\frac{\partial out_{O1}}{\partial net_{O1}}=0.74136507*0.186815602=0.138498562
\end{equation}</script>

<p>又因为$\frac{\partial net_{O1}}{\partial out_{h1}}$等于$w_{5}$:</p>

<script type="math/tex; mode=display">\begin{equation}
   net_{O1}=w_5*out_{h1}+w_6*out_{h2}+b2*1\\\\
   \frac{\partial net_{O1}}{\partial out_{h1}}=w_5=0.40
\end{equation}</script>

<p>将上面每步分开算的结果合起来得：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{O1}}{\partial out_{h1}}=\frac{\partial E_{O1}}{\partial net_{O1}}*\frac{\partial net_{O1}}{\partial out_{h1}}=0.138498562*0.40=0.055399425
\end{equation}</script>

<p>与上面的步骤一样，我们可以得到：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{O2}}{\partial out_{h1}}=w_5=-0.019049119
\end{equation}</script>

<p>因此：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial out_{h1}}=\frac{\partial E_{O1}}{\partial out_{h_1}}+\frac{\partial E_{O2}}{\partial out_{h1}}=0.055399425+(-0.019049119)=0.036350306
\end{equation}</script>

<p>现在我们已经有了$\frac{\partial E_{total}}{\partial out_{h1}}$，我们还需要为每一个需要更新的权重计算$\frac{\partial out_{h1}}{\partial net_{h1}}$和$\frac{\partial net_{h1}}{\partial w}$：</p>

<script type="math/tex; mode=display">\begin{equation}
   out_{h1}=\frac{1}{1+e^(net_{h1})}\\\\
   \frac{\partial out_{h1}}{\partial net_{h1}}=out_{h1}(1-out_{h1})=0.59326999(1-0.59326999)=0.241300709
\end{equation}</script>

<p>如我们前面对于输出神经元所做的一样，我们计算$h1$的全部输入关于$w_1$求偏导：</p>

<script type="math/tex; mode=display">\begin{equation}
   net_{h1}=w_1*i_1+w_2*i_2+b_1*1\\\\
   \frac{\partial net_{h1}}{\partial w_{1}}=i_1=0.05
\end{equation}</script>

<p>将上面计算的各个部分合起来：</p>

<script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial w_{1}}=\frac{\partial E_{total}}{\partial out_{h1}}*\frac{\partial out_{h1}}{\partial net_{h1}}*\frac{\partial net_{h1}}{\partial w_{1}}\\\\
      \frac{\partial E_{total}}{\partial w_{1}}=0.036350306*0.241300709*0.05=0.000438568
\end{equation}</script>

<blockquote>
  <p>你可以可能会看到下面的这种写法：</p>

  <script type="math/tex; mode=display">\begin{equation}
   \frac{\partial E_{total}}{\partial w_{1}}=(\sum_O \frac{\partial E_{total}}{\partial out_{O}}*\frac{\partial out_{O}}{\partial net_{O}}*\frac{\partial net_{O}}{\partial out_{h1}})*\frac{\partial out_{h1}}{\partial net_{h1}}*\frac{\partial net_{h1}}{\partial w_{1}}\\\\
   \frac{\partial E_{total}}{\partial w_{1}}=(\sum_O \delta_O*w_{hO})*out_{h1}(1-out_{h1})*i\\\\
   \frac{\partial E_{total}}{\partial w_{1}}=\delta_{h1}i_1
\end{equation}</script>
</blockquote>

<p>现在我们可以更新$w_1$:</p>

<script type="math/tex; mode=display">\begin{equation}
w_1^+=w_1-\eta*\frac{\partial E_{total}}{\partial w_{1}}=0.15-0.5*0.000438568=0.149780716
\end{equation}</script>

<p>根据上面相同的计算过程，我们可以得到$w_2$、$w_3$和$w_4$：</p>

<script type="math/tex; mode=display">\begin{equation}
w_2^+=0.19956143
w_3^+=0.24975114
w_4^+=0.29950229
\end{equation}</script>

<p>现在，我们已经更新了所有的权重，在最初，在我们的输入为0.05和0.1的时候，网络的误差为0.298371109， 经过第一次方向传播后，网络的误差降低到了0.291027924。虽然看起来下降得不是很多，但是在重复这个过程10000次以后，网络的误差就下降到了0.000035085。这个时候，当我们把0.05和0.1再输入进去，两个神经元的输出为0.015912196(vs 0.01)和0.984065734(vs 0.99)。</p>

<p>如果你在读上面的博文的时候发现有任何错误，不要犹豫告诉我。如果你有更清楚的想读者讲解的方式，不要犹豫给我留言。谢谢！</p>

  	</div>
	</div>

<div class="content-play">
    <p><a href="javascript:void(0)" onclick="dashangToggle()" class="dashang" title="打赏，支持一下">请我喝杯咖啡</a></p>
    <div class="hide_box-play"></div>
    <div class="shang_box-play">
        <a class="shang_close-play" href="javascript:void(0)" onclick="dashangToggle()" title="关闭"><img src="/images/payimg/close.jpg" alt="取消" /></a>
        <div class="shang_tit-play">
            <p>感谢您的支持，我会继续写出更优秀的文章!</p>
        </div>
        <div class="shang_payimg">
            <img src="/images/payimg/weipayimg.jpg" alt="扫码支持" title="扫一扫" />
        </div>
        <div class="shang_payimg">
            <img src="/images/payimg/alipayimg.jpg" alt="扫码支持" title="扫一扫" />
        </div>
        <div class="pay_explain">请我喝杯咖啡</div>
        <div class="shang_payselect">
            <div class="pay_item" data-id="weipay">
                <span class="pay_logo"><img src="/images/payimg/wechat.jpg" alt="微信" /></span>
            </div>
            <div class="pay_item checked" data-id="alipay">
                <span class="pay_logo"><img src="/images/payimg/alipay.jpg" alt="支付宝" /></span>
            </div>
        </div>
        <div class="shang_info-play">
            <p>打开<span id="shang_pay_txt">微信</span>扫一扫，即可进行扫码打赏哦</p>
        </div>
    </div>
</div>
<script type="text/javascript">
    function dashangToggle(){
        $(".hide_box-play").fadeToggle();
        $(".shang_box-play").fadeToggle();
    }
</script>

<div style="text-align:center;margin:50px 0; font:normal 14px/24px 'MicroSoft YaHei';"></div>

<style type="text/css">
  .content-play{}
  .hide_box-play{z-index:999;filter:alpha(opacity=50);background:#666;opacity: 0.5;-moz-opacity: 0.5;left:0;top:0;height:99%;width:100%;position:fixed;display:none;}
  .shang_box-play{width:540px;height:540px;padding:10px;background-color:#fff;border-radius:10px;position:fixed;z-index:1000;left:50%;top:50%;margin-left:-280px;margin-top:-280px;border:1px dotted #dedede;display:none;}
  .shang_box-play img{border:none;border-width:0;}
  .dashang{display:block;width:140px;margin:5px auto;height:40px;line-height:25px;padding:8px;background-color:#E74851;color:#fff;text-align:center;text-decoration:none;border-radius:10px;font-weight:bold;font-size:16px;transition: all 0.3s;}
  .dashang:hover{opacity:0.8;padding:15px;font-size:18px;}
  .shang_close-play{float:right;display:inline-block;
    margin-right: 10px;margin-top: 20px;
  }
  .shang_logo{display:block;text-align:center;margin:20px auto;}
  .shang_tit-play{width: 100%;height: 75px;text-align: center;line-height: 66px;color: #a3a3a3;font-size: 16px;background: url('/images/payimg/cy-reward-title-bg.jpg');font-family: 'Microsoft YaHei';margin-top: 7px;margin-right:2px;}
  .shang_tit-play p{color:#a3a3a3;text-align:center;font-size:16px;}
  .shang_payimg{width:140px;padding:10px;/*padding-left: 80px; border:6px solid #EA5F00;**/margin:0 auto;border-radius:3px;height:140px;display:inline-block;}
  .shang_payimg img{display:inline-block;margin-right:10px;float:left;text-align:center;width:140px;height:140px; }
  .pay_explain{text-align:center;margin:10px auto;font-size:12px;color:#545454;}
  .shang_payselect{text-align:center;margin:0 auto;margin-top:40px;cursor:pointer;height:60px;width:500px;margin-left:110px;}
  .shang_payselect .pay_item{display:inline-block;margin-right:140px;float:left;}
  .shang_info-play{clear:both;}
  .shang_info-play p,.shang_info-play a{color:#C3C3C3;text-align:center;font-size:12px;text-decoration:none;line-height:2em;}
</style>


</article>

<!--翻页功能-->
<section class="inner">
<ul class="pager">
    
    <li class="previous">
       <a href="/blog/SGD-01.html">←机器学习：随机梯度下降法</a>
    </li>
    
    
    <li class="next">
    	<a href="/blog/boost-serialization.html">机器视觉：特征序列化→</a>
    </li>
    
 </ul>
</section>

<!--评论功能-->
<section class="comments inner duoshuo">
<!-- Disqus Comment BEGIN -->
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'yongyuan'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
<!-- Disqus Comment END -->

</section>
</div>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    <!--displayMath: [['$$','$$'], ['\[','\]']],-->
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>



  <!--<div class="separator"></div>-->

  <footer>
    <div  style="border-bottom: 1px solid #eee"></div>
    <p class="linkings">
    Friend: <a href="https://sites.google.com/site/raphaelitemou/">Lichao</a>&nbsp&nbsp&nbsp&nbsp<a href="http://cmp.felk.cvut.cz/~toliageo/publi.html">Tolias</a>&nbsp&nbsp&nbsp&nbsp<a href="http://www.heyuhang.com/">heyuhang</a>&nbsp&nbsp&nbsp&nbsp<a href="https://blog.yuanbin.me/">yuanbin</a>&nbsp&nbsp&nbsp&nbsp<a href="http://freemind.pluskid.org/">pluskid</a>
    </p>
    <!--p class="linkings">
        Utils: <a href="http://rogerioferis.com/VisualRecognitionAndSearch2014/Resources.html">rogerioferis</a>&nbsp&nbsp&nbsp&nbsp<a href="http://caffe.berkeleyvision.org/">Caffe</a>&nbsp&nbsp&nbsp&nbsp<a href="http://scikit-learn.org/stable/index.html">scikit-learn</a>&nbsp&nbsp&nbsp&nbsp<a href="http://www.astroml.org/index.html">AstroML</a></a>&nbsp&nbsp&nbsp&nbsp<a href="http://scikit-image.org/">scikit-image</a></a>&nbsp&nbsp&nbsp&nbsp<a href="http://gitxiv.com/">GitXiv</a>
    </p-->
    <p>
      Made with <a href="http://jekyllrb.com/">Jekyll</a>,
      hosted on <a href="https://github.com/willard-yuan/willard-yuan.github.io">Github Pages</a>. Inspired by <a href="http://sebastien.saunier.me/">saunier</a>, designed by <a href="http://yongyuan.name">Willard</a>.
    </p>
    <p>
      <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial-ShareAlike 4.0 International</a> 2013-2018
    </p>
    <!--ul class="links">
      <li>
        <a href="https://github.com/willard-yuan" title="See my code on GitHub">
          <i class="icon-github"></i>
        </a>
      </li>
      <li>
        <a href="https://twitter.com/intent/follow?screen_name=yongyuan001" title="Follow me on Twitter">
          <i class="icon-twitter"></i>
        </a>
      </li>
      <li>
        <a href="https://www.linkedin.com/in/sebastiensaunier" title="Connect with me on LinkedIn">
          <i class="icon-linkedin"></i>
        </a>
      </li>
      <li>
        <a href="https://plus.google.com/+SebastienSaunier?rel=author" title="I need this link for authorship reasons">
          <i class="icon-google-plus"></i>
        </a>
      </li>
      <li>
        <a href="https://pinboard.in/u:ssaunier" title="Browse my bookmarks about Programming">
          <i class="icon-bookmarks"></i>
        </a>
      </li>
      <li>
        <a href="/feed.xml" title="Subscribe to my blog with RSS">
          <i class="icon-feed"></i>
        </a>
      </li>
    </ul-->
  </footer>

<script src='http://cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js'></script>

</body>
</html>
