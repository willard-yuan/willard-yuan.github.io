<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>YongYuan's homepage</title>
    <description>I am Yong Yuan. Willard is my English Name, which means great bravery. I completed my bachelor's degree in Faculty of Science at Xi'Dian University of science and technology of the electronics information on July 1, 2013.</description>
    <link>http://localhost:4000</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>图像检索：OPQ索引与HNSW索引</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;人的独立性和参与性必须适得其所，平衡发展。一方面，过分的参与必然导致远离自我核心，现代人之所以感到空虚、无聊，在很大程度上就是由于顺从、依赖和参与过多，脱离了自我核心。另一方面，过分的独立会将自己束缚在狭小的自我世界内，缺乏正常的交往，必然损害人的正常发展。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;关于索引结构，有千千万万，而在图像检索领域，索引主要是为特征索引而设计的一种数据结构。关于ANN搜索领域的学术研究，&lt;a href=&quot;http://www.itu.dk/people/pagh/&quot;&gt;Rasmus Pagh&lt;/a&gt;发起的大规模相似搜索项目&lt;a href=&quot;http://sss.projects.itu.dk/ann-benchmarks/&quot;&gt;ANN-Benchmarks&lt;/a&gt;、&lt;a href=&quot;https://github.com/facebookresearch/faiss&quot;&gt;Faiss&lt;/a&gt;以及&lt;a href=&quot;https://github.com/erikbern/ann-benchmarks&quot;&gt;ann-benchmarks&lt;/a&gt;都有对一些主流的方法做过对比。虽然三个对比的框架对不同方法的性能均有出入，但一些主流方法的性能差异是可以达成共识的，比如基于图方法的ANN其召回率均要优于其他方法。在工业上，常用的索引方法主要以倒排、&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;PQ及其变种&lt;/a&gt;、基于树的方法（比如KD树）和&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;哈希&lt;/a&gt;（典型代表LSH和&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;ITQ&lt;/a&gt;）为主流。关于KD树、LSH以及PQ，小白菜曾在此前的博文&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;已有比较详细的介绍。本文是小白菜结合实际应用，对PQ的改进方法OPQ以及基于图的方法HNSW的理解，以及关于索引的一些总结与思考。&lt;/p&gt;

&lt;h2 id=&quot;opq-vs-hnsw&quot;&gt;OPQ vs. HNSW&lt;/h2&gt;

&lt;p&gt;首先从检索的召回率上来评估，基于图的索引方法要优于目前其他一些主流ANN搜索方法，比如乘积量化方法（PQ、OPQ）、哈希方法等。虽然乘积量化方法的召回率不如HNSW，但由于乘积量化方法具备内存耗用更小、数据动态增删更灵活等特性，使得在工业检索系统中，在对召回率要求不是特别高的场景下，乘积量化方法仍然是使用得较多的一种索引方法，淘宝（详见&lt;a href=&quot;https://arxiv.org/abs/1707.00143&quot;&gt;Fast Approximate Nearest Neighbor Search With The Navigating Spreading-out Graph&lt;/a&gt;）、蘑菇街等公司均有使用。乘积量化和HNSW特性对比如下：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特性&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;OPQ&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;HNSW&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;内存占用&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;小&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;大&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;召回率&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;较高&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;高&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;数据动态增删&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;灵活&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;不易&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;基于图ANN方法由于数据在插入索引的时候，需要计算（部分）数据间的近邻关系，因而需要实时获取到到数据的原始特征，几乎所有基于图ANN的方法在处理该问题的时候，都是直接将原始特征加载在内存（索引）里，从而造成对内存使用过大，至于召回率图ANN方法要比基于量化的方法要高，这个理解起来比较直观。下面分别对改进的乘积量化方法OPQ以及基于图ANN方法HNSW做原理上的简要介绍。&lt;/p&gt;

&lt;h2 id=&quot;opq&quot;&gt;OPQ&lt;/h2&gt;

&lt;p&gt;OPQ是PQ的一种改进方法，关于PQ的介绍，在此前的文章&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;中已有详细介绍，这里仅对改进的部分做相应的介绍。&lt;/p&gt;

&lt;p&gt;通常，用于检索的原始特征维度较高，所以实际在使用PQ等方法构建索引的时候，常会对高维的特征使用PCA等降维方法对特征先做降维处理，这样降维预处理，可以达到两个目的：一是降低特征维度；二是在对向量进行子段切分的时候要求特征各个维度是不相关的，做完PCA之后，可以一定程度缓解这个问题。但是这么做了后，在切分子段的时候，采用顺序切分子段仍然存在一定的问题，这个问题可以借用&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;ITQ&lt;/a&gt;中的一个二维平面的例子加以说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://yongyuan.name/images/posts/ITQ_hashing.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上面左图（a图）所示，对于PCA降维后的二维空间，假设在做PQ的时候，将子段数目设置为2段，即切分成$x$和$y$两个子向量，然后分别在$x$和$y$上做聚类（假设聚类中心设置为2）。对左图（a图）和右图（c图）聚类的结果进行比较，可以明显的发现，左图在y方向上聚类的效果明显差于右图，而PQ又是采用聚类中心来近似原始向量（这里指降维后的向量），也就是右图是我们需要的结果。这个问题可以转化为数据方差来描述：&lt;strong&gt;在做PQ编码时，对于切分的各个子空间，我们应尽可能使得各个子空间的方差比较接近，最理想的情况是各个子空间的方差都相等&lt;/strong&gt;。上图左图中，$x$和$y$各个方向的方差明显是差得比较大的，而对于右图，$x$和$y$方向各个方向的方差差不多是比较接近的。&lt;/p&gt;

&lt;p&gt;为了在切分子段的时候，使得各个子空间的方差尽可能的一致，&lt;a href=&quot;https://research.fb.com/people/jegou-herve/&quot;&gt;Herve Jegou&lt;/a&gt;在&lt;a href=&quot;https://lear.inrialpes.fr/pubs/2010/JDSP10/jegou_compactimagerepresentation.pdf&quot;&gt;Aggregating local descriptors into a compact image representation&lt;/a&gt;中提出使用一个正交矩阵来对PCA降维后的数据再做一次变换，使得各个子空间的方差尽可能的一致。其对应的待优化目标函数见论文的第5页，由于优化该目标函数极其困难，Herve Jegou使用了Householder矩阵来得到该正交矩阵，但是得到的该正交矩阵并不能很好的均衡子空间的方差。&lt;/p&gt;

&lt;p&gt;OPQ致力于解决的问题正是对各个子空间方差的均衡。具体到方法上，OPQ借鉴了ITQ的思想，在聚类的时候对聚类中心寻找对应的最优旋转矩阵，使得所有子空间中各个数据点到对应子空间的类中心的L2损失的求和最小。OPQ在具体求解的时候，分为非参求解方法和带参求解方法，具体为：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;非参求解方法。跟ITQ的求解过程一样。&lt;/li&gt;
  &lt;li&gt;带参求解方法。带参求解方法假设数据服从高斯分布，在此条件下，最终可以将求解过程简化为数据经过PCA分解后，特征值如何分组的问题。在实际中，该解法更具备高实用性。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hnsw&quot;&gt;HNSW&lt;/h2&gt;

&lt;p&gt;HNSW是Yury A. Malkov提出的一种基于图索引的方法，它是Yury A. Malkov在他本人之前工作NSW上一种改进，通过采用层状结构，将边按特征半径进行分层，使每个顶点在所有层中平均度数变为常数，从而将NSW的计算复杂度由多重对数(Polylogarithmic)复杂度降到了对数(logarithmic)复杂度。&lt;/p&gt;

&lt;h3 id=&quot;贡献&quot;&gt;贡献&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;图输入节点明确的选择&lt;/li&gt;
  &lt;li&gt;使用不同尺度划分链接&lt;/li&gt;
  &lt;li&gt;使用启发式方式来选择最近邻&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;近邻图技术&quot;&gt;近邻图技术&lt;/h3&gt;

&lt;p&gt;对于给定的近邻图，在开始搜索的时候，从若干输入点（随机选取或分割算法）开始迭代遍历整个近邻图。&lt;/p&gt;

&lt;p&gt;在每一次横向迭代的时候，算法会检查链接或当前base节点之间的距离，然后选择下一个base节点作为相邻节点，使得能最好的最小化连接间的距离。&lt;/p&gt;

&lt;p&gt;近邻图主要的缺陷：1. 在路由阶段，如果随机从一个或者固定的阶段开始，迭代的步数会随着库的大小增长呈现幂次增加；2. 当使用k-NN图的时候，一个全局连接可能的损失会导致很差的搜索结果。&lt;/p&gt;

&lt;h3 id=&quot;算法描述&quot;&gt;算法描述&lt;/h3&gt;

&lt;p&gt;网络图以连续插入的方式构建。对于每一个要插入的元素，采用指数衰变概率分布函数来随机选取整数最大层。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2018/0715/hnsw.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;图构建元素插入过程（Algorithm 1）：从顶层开始贪心遍历graph，以便在某层A中找到最近邻。当在A层找到局部最小值之后，再将A层中找到的最近邻作为输入点（entry point），继续在下一层中寻找最近邻，重复该过程；&lt;/li&gt;
  &lt;li&gt;层内最近邻查找（Algorithm 2）：贪心搜索的改进版本；&lt;/li&gt;
  &lt;li&gt;在搜索阶段，维护一个动态列表，用于保持ef个找到的最近邻元素&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在搜索的初步阶段，ef参数设置为1。搜索过程包括zoom out和zoom in两个阶段，zoom out是远程路由，zoom in顾名思义就是在定位的区域做精细的搜索过程。整个过程可以类比在地图上寻找某个位置的过程：我们可以地球当做最顶层，五大洲作为第二层，国家作为第三层，省份作为第四层……，现在如果要找海淀五道口，我们可以通过顶层以逐步递减的特性半径对其进行路由（第一层地球-&amp;gt;第二层亚洲—&amp;gt;第三层中国-&amp;gt;第四层北京-&amp;gt;海淀区），到了第0层后，再在局部区域做更精细的搜索。&lt;/p&gt;

&lt;h3 id=&quot;数据实验说明&quot;&gt;数据实验说明&lt;/h3&gt;

&lt;p&gt;199485332条人脸数据（128维，L2归一化）作为database, 10000条人脸数据作为查询。gound truth由暴力搜索结果产生（余弦相似度），将暴力搜索结果的rank@1作为gound truth，评估top@K下的召回率。&lt;/p&gt;

&lt;h3 id=&quot;实验结果与调优&quot;&gt;实验结果与调优&lt;/h3&gt;

&lt;p&gt;M参数：80，内存大小: 159364 Mb，索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_80_M_32_ip.bin&lt;/code&gt;，查询样本数目: 10000，ef: 1000，距离：内积距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time(us) per query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.977300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9754.885742us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.981200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9619.380859us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.983100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9652.819336us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.983800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9628.488281us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.984500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9650.678711us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.986400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9647.286133us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.986700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9665.638672us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9685.414062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9744.437500us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.987100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9804.702148us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/notes/hnsw_face_2b.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;M参数：16，Mem: 173442 Mb， 索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_40_M_16.bin&lt;/code&gt;, 查询样本数目: 10000，ef: 1000，距离：欧氏距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time_us_per_query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.887800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4845.700684us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.911700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6732.230957us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.916600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6879.585449us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.917500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6963.914062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.918000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6920.318359us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.920200&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6880.795898us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.922400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6900.778809us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6970.664062us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6978.517578us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.923400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6992.306152us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;M参数：40，Mem: 211533 Mb， 索引文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;cnn2b_199485332m_ef_40_M_40.bin&lt;/code&gt;, 查询样本数目: 10000，ef: 1000，距离：内积距离&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;top@K&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;召回&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;时间(time_us_per_query)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.928600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6448.714355us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.948300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7658.459961us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.952600&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7674.244629us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.954000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7659.506348us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.954700&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7679.874023us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.955800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7709.812500us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957400&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7720.283691us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.957800&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7722.512695us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;300&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7763.615234us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;500&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7779.351562us&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1000&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.958100&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7859.372559us&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;余弦相似度与余弦距离关系&quot;&gt;余弦相似度与余弦距离关系&lt;/h3&gt;

&lt;p&gt;Supported distances:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Distance&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;parameter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Equation&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Squared L2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘l2’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = sum((Ai-Bi)^2)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Inner product&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘ip’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = 1.0 - sum(Ai*Bi))&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Cosine similarity&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;‘cosine’&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;d = 1.0 - sum(Ai*Bi) / sqrt(sum(Ai*Ai) * sum(Bi*Bi))&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;参数说明&quot;&gt;参数说明&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;efConstruction：设置得越大，构建图的质量越高，搜索的精度越高，但同时索引的时间变长，推荐范围100-2000&lt;/li&gt;
  &lt;li&gt;efSearch：设置得越大，召回率越高，但同时查询的响应时间变长，推荐范围100-2000，在HNSW，参数ef是efSearch的缩写&lt;/li&gt;
  &lt;li&gt;M：在一定访问内，设置得越大，召回率增加，查询响应时间变短，但同时M增大会导致索引时间增加，推荐范围5-100&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;HNSW L2space返回的top@K，是距离最小的K个结果，但是在结果表示的时候，距离是从大到小排序的，所以top@K距离是最小的，top@K-1距离是次之，top@1是距离第K大的。只是结果在表示上逆序了而已，不影响最终的结果。如果要按正常的从小到大来排序，则对top@K的结果做个逆序即可。作者在python的接口里，实现了这种逆序，具体见&lt;a href=&quot;https://github.com/nmslib/hnsw/blob/master/python_bindings/bindings.cpp#L287&quot;&gt;bindings.cpp#L287&lt;/a&gt;，所以python的结果和c++的结果，是逆序的差异。&lt;/p&gt;

&lt;h3 id=&quot;参数详细意义&quot;&gt;参数详细意义&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;M：参数M定义了第0层以及其他层近邻数目，不过实际在处理的时候，第0层设置的近邻数目是2*M。如果要更改第0层以及其他层层近邻数目，在HNSW的源码中进行更改即可。另外需要注意的是，第0层包含了所有的数据点，其他层数据数目由参数mult定义，详细的细节可以参考HNSW论文。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;delaunay_type：检索速度和索引速度可以通过该参数来均衡heuristic。HNSW默认delaunay_type为1，将delaunay_type设置为1可以提高更高的召回率(&amp;gt; 80%)，但同时会使得索引时间变长。因此，对于召回率要求不高的场景，推荐将delaunay_type设置为0。&lt;/li&gt;
  &lt;li&gt;post：post定义了在构建图的时候，对数据所做预处理的数量（以及类型），默认参数设置为0，表示不对数据做预处理，该参数可以设置为1和2（2表示会做更多的后处理）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;更详细的参数说明，可以参考&lt;a href=&quot;https://github.com/nmslib/nmslib/blob/9ed3071d0a74156a9559f3347ee751922e4b06e7/python_bindings/parameters.md&quot;&gt;parameters说明&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&quot;demo&quot;&gt;demo&lt;/h3&gt;

&lt;p&gt;小白菜基于局部特征，采用HNSW做过一版实例搜索，详细说明详见&lt;a href=&quot;https://github.com/willard-yuan/cvtk/tree/master/hnsw_sifts_retrieval&quot;&gt;HNSW SIFTs Retrieval&lt;/a&gt;。适用范围：中小规模。理论上，直接基于局部特征索引的方法，做到上千万级别的量级，是没有问题的，成功的例子详见&lt;a href=&quot;http://flickrdemo.videntifier.com/&quot;&gt;videntifier&lt;/a&gt;，&lt;a href=&quot;http://www.videntifier.com/about&quot;&gt;Herwig Lejsek&lt;/a&gt;在设计videntifier系统的时候，使用的是NV-Tree，每一个高维向量只需用6个字节来表示，压缩比是非常大的，(O)PQ折中情况下一般都需要16个字节。关于NV-Tree的详细算法，可以阅读&lt;a href=&quot;http://www.videntifier.com/about&quot;&gt;Herwig Lejsek&lt;/a&gt;的博士论文&lt;a href=&quot;https://en.ru.is/media/skjol-td/PhDHerwig.pdf&quot;&gt;NV-tree: A Scalable Disk-Based High-Dimensional Index&lt;/a&gt;，墙裂推荐一读。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;在本篇博文里，小白菜对图ANN、基于量化的两类方法分别选取了最具代表性的方法HNSW和OPQ方法进行比较详细的总结，其中由以基于PQ的量化方法在工业界最为实用，基于图的ANN方法，在规模不是特别大但对召回要求非常高的检索场景下，是非常适用的。除此之外，图ANN方法可以和OPQ结合起来适用，来提高OPQ的召回能力，具体可以阅读&lt;a href=&quot;https://arxiv.org/abs/1802.02422&quot;&gt;Revisiting the Inverted Indices for Billion-Scale Approximate Nearest Neighbors&lt;/a&gt;和&lt;a href=&quot;https://arxiv.org/abs/1804.09996&quot;&gt;Link and code: Fast indexing with graphs and compact regression codes&lt;/a&gt;这两篇文章。&lt;/p&gt;
</description>
        <pubDate>Sun, 15 Jul 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/opq-and-hnsw.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/opq-and-hnsw.html</guid>
      </item>
    
      <item>
        <title>图像检索：视频多帧排序</title>
        <description>&lt;h2 id=&quot;背景与问题&quot;&gt;背景与问题&lt;/h2&gt;

&lt;p&gt;每一个视频抽取n帧，n是变化的，有的视频长抽的帧数多，有的视频短抽的帧数相应的也少一些。在索引的时候，将所有视频的帧都索引在一起。对于查询的视频，同样抽取视频帧，假设抽取到了m帧，那么问题来了，对这m帧的查询结果，其排序逻辑该如何设计？&lt;/p&gt;

&lt;h2 id=&quot;多帧相似性度量&quot;&gt;多帧相似性度量&lt;/h2&gt;

&lt;p&gt;对于文章开头提出的问题，可以先对其进行简化，先思考这样一个问题：&lt;strong&gt;两个视频&lt;/strong&gt;，如何度量两个视频的相似性（引申问题：如果校验两个图片或两个视频是重复的）？&lt;/p&gt;

&lt;p&gt;如果两个视频，其提取的特征是视频维度的全局特征，即每个视频最终表示成了一个全局向量表示（比如iDT、C3D等特征），那么度量两个视频间的相似性时，可以直接计算它们之间的余弦相似性或者欧式距离等。这样固然最省事，但在实际应用中，当视频存量上亿，并且还有源源不断的视频添加进来时，显然基于视频维度提特征的方式因为耗费资源太大而无法得以在具备实时性要求的场景中加以应用。因而实际中在对视频内容做相关理解与分析的时候，往往是基于抽帧的方式（如何抽帧以及是否做关键帧检测在此不表）。在获取到视频的帧后，可以在此基础上提取视频维度的特征，或者提取帧级别的特征。&lt;/p&gt;

&lt;p&gt;小白菜简单地总结视频维度特征与帧级别特征（类似全局特征和局部特征）的优劣：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;视频维度特征：考虑了时空维度信息，有利于表述整个视频发生的事件等全局信息，表达的特征更紧凑；&lt;/li&gt;
  &lt;li&gt;帧级别特征：最大的缺陷在于未考虑视频时间维的信息，将帧与帧之间孤立起来了。当然帧级别的特征，其对视频空间信息的特征描述的力度更细一些，在某些场景中，比如校验两个视频是否是重复视频，基于帧级别的特征反而更适用；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般而言，抽取视频维度的特征通常是非常耗时的。因而，实际面向视频的应用，除非场景应用非常有价值且规模不大可能会使用视频维度的特征，大部分的视频内容理解往往是基于帧级别的特征。现在回到本节开头的问题：两个视频，如何度量两个视频的相似性？这个问题小白菜曾在&lt;a href=&quot;http://yongyuan.name/blog/asymmetry-problem-in-computer-vision.html&quot;&gt;机器视觉：Asymmetry Problem in Computer Vision&lt;/a&gt;中介绍累积最小（最大）距离非对称问题时有涉及过，可以看出，累积最小（最大）距离极其符合该应用场景。因而，对于两个视频，要度量两个视频的相似性，可以采用累积最小（最大）距离，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
S(X, Y) = \sum_{i=1}^{i=n} \sum_{j=1}^{j=m} d_\min(x_i, y_j)
\end{equation}&lt;/p&gt;

&lt;p&gt;累积最小（最大）距离在应用的时候，可能会出现A视频中的多个帧匹配到B视频中的一帧或交叉匹配的情况（视频序列是时序的）。对于这种情况，在算帧匹配的时候，可以使用动态规划（DP）方法避免出现多帧和一帧匹配或交叉匹配，并确保得到的$S(X,Y)$又是最小的。当然，直接计算累积最小（最大）距离也依然是很好的。实际应用中，这种基于累积最小（最大）距离的多帧相似度度量经受住了应用的考验，而且计算效率比采用DP避免交叉匹配方式更高，并且效果很理想。&lt;/p&gt;

&lt;p&gt;有了两个视频间多帧相似性的度量，将其拓展到一个query视频跟N个视频的相似性排序，就相比比较直观了。下面分别对暴力搜索多帧排序和OPQ多帧排序予以介绍，OPQ多帧排序是暴力搜索多帧排序的继承，只不过在对每帧查询的时候，使用的是OPQ索引，而暴力搜索则是直接query。&lt;/p&gt;

&lt;h2 id=&quot;暴力搜索多帧排序&quot;&gt;暴力搜索多帧排序&lt;/h2&gt;

&lt;p&gt;在不需要实时处理、对召回要求很高并且不是很频繁查询的场景，比如回溯任务中，暴力搜索仍然是一种值得推荐的搜索方法。原因有二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一是特征性能发挥到最大，从而保证最好的召回率；&lt;/li&gt;
  &lt;li&gt;二是在特征维度不是很高的情况下（几百维，如果是CNN特征且维度较高，比如上千维，可以降到128d或者512d，损失精度通常很小），通过SSE加速，距离的计算也可以算得很快。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;对于多帧排序，其排序逻辑建立在&lt;strong&gt;两个视频&lt;/strong&gt;多帧相似性度量上，即对于一个query视频，库中的每个视频在与query视频做相似性度量的时候，均应采用上一节指出的&lt;strong&gt;累积最小（最大）距离&lt;/strong&gt;分别计算相似度。在实际计算的时候，这种逐个视频计算的方式可以优化为query视频与N个视频进行内积运算（特征在提取的时候进行了$L2$归一化）的方式，然后对相似性矩阵做排序处理，假设query视频帧数是$m$帧，库中共$N$个视频，每一个视频取$n$帧，则相似性矩阵大小为$m \times (N*n)$，即每一行对应query视频某一帧查询后排序的结果，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2018/0516/multiframes_reranking1.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图中，qF1表示查询视频的第1帧，AF1表示视频A的第1帧，后面以此类推。对该排完序后的相似性矩阵，我们需要对每$i(i = 0,…,m)$行取出视频id对应的最大的值（&lt;strong&gt;因为计算的是余弦相似度&lt;/strong&gt;）作为该帧对应各视频id的结果，这个过程可以用下图来直观的表述出来：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2018/0516/multiframes_reranking2.png&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，以qF1帧查询为例，由于已经对该帧查询的结果按相似性进行的排序，因而qF1帧查询对A视频查询的结果为AF1，其余以此类推。最后对这m帧经过最大值筛选后的结果，按视频id进行相似性分数的归并，最终得到多帧相似性排序分数，即：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A视频分数：AF1+AF2+AF1+...+AF3 
B视频分数：BF1+BF1+BF1+...+BF1
C视频分数：CF1+CF3+0.0+...+0.0
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面假设C视频对于查询视频的每帧，在设置得阈值条件下无返回结果，因而用分数0.0来表示（可以视为将所有的结果在初始化的时候初始化为0）。实际query的时候，对于每一帧的查询结果，不需要将所有的查询结果返回回来，可以通过设置一个相似度阈值只返回一定数目的结果，相似度太小的返回来了也没什么用，徒增计算量和浪费资源，这样处理后排序、筛选、合并的计算量大大降低了。&lt;/p&gt;

&lt;h2 id=&quot;opq多帧排序&quot;&gt;OPQ多帧排序&lt;/h2&gt;

&lt;p&gt;在对实时性有要求的场景，比如上传一个视频，需要从海量的视频库中召回相同或者相似的那些视频，显然暴力搜索多帧排序无法满足要求，需要以某种近似最近邻搜索方法来构建索引。关于近似最近邻搜索方法，在此前的博文如&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;以及其他的博文中有过很多的介绍，这里仅以OPQ为例来介绍ANN多帧排序方法。&lt;/p&gt;

&lt;p&gt;OPQ多帧排序方法和暴力搜索多帧排序都是基于多帧相似性度量，不同的是，由于OPQ（或PQ）计算的非对称距离，其算出来的值越小，表示两者越相似，而暴力搜索采用的是余弦相似度，其值越大表示两者越相似。因而，对于OPQ多帧排序逻辑，在设计的时候，似乎只需要为排序初始化的分数初始化一个比较合理的值，这里只对返回所有结果做初始化，下面是详细的OPQ多帧排序。&lt;/p&gt;

&lt;p&gt;对于某查询视频queryVid，假设queryVid有m帧$（f_1,f_2,…,f_m）$，对于第i帧查询，在距离阈值小于0.1的条件下，返回了k个结果$(r_1, r_2,…,r_k)$，然后对这m帧的结果按视频id进行最小值选取与合并。举个简单的例子，假设某次查询视频只有3帧，下面是每帧查询返回的结果：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;查询视频第f1帧返回的结果: A1, B1, E1, C1, D1, B2, E2
查询视频第f2帧返回的结果: B1, C2, D1, C1, E1
查询视频第f3帧返回的结果: A2, C1, B2, D2, E1, D1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于查询视频第f1帧返回的结果，假设$A1 \leqslant B1 \leqslant E1 \leqslant C1 \leqslant D1 \leqslant B2 \leqslant E2 \leqslant 1.0 $，后面查询的同理。对上面每帧查询的结果，按视频id取最小值，得下面结果：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;查询视频第f1帧返回的结果: A1, B1, E1, C1, D1
查询视频第f2帧返回的结果: B1, C2, D1, E1
查询视频第f3帧返回的结果: A2, C1, B2, D2, E1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;对于第f2帧返回的结果，给A视频我们置一个比较大的阈值，比如上面提过的置为1.0来代替，表示第f2帧作为查询时，与A视频相似度差得比较大。之后，便可按视频id对结果进行合并了：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A视频分数：A1+1.0+A2
B视频分数：B1+B1+B2   
C视频分数：C1+C2+C1
D视频分数：D1+D1+D2
E视频分数：E1+E1+E1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;然后再对A、B、C、D、E视频上面求和后的分数从小到大排序，得到的便是最终多帧查询排序的结果。整个过程在实现的时候，可以采用&lt;code class=&quot;highlighter-rouge&quot;&gt;partial_sort&lt;/code&gt;方法来完成，关于&lt;code class=&quot;highlighter-rouge&quot;&gt;partial_sort&lt;/code&gt;的用法，可以参考&lt;a href=&quot;http://www.cnblogs.com/qlee/archive/2011/05/25/2057281.html&quot;&gt;理解你的排序操作&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;无论是对于暴力搜索多帧排序、还是OPQ多帧排序，在采用了上述介绍的累积最小（最大）距离的多帧相似度度量后，跟之前采用的基于倒排查询相比，查询效果取得了极大的提升。另外对于OPQ多帧排序的结果，还可以对前top@K的结果做一次重排，能够将累积最小（最大）距离的多帧相似度度量的检索准确率发挥到最大的程度。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;在本篇博文中，小白菜以多帧索引及排序为主题进行了一些讨论，包括了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;多帧相似性度量&lt;/li&gt;
  &lt;li&gt;暴力搜索多帧排序&lt;/li&gt;
  &lt;li&gt;OPQ多帧排序&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;3个方面的内容，这些内容在视频搜索上一般都会涉及。关于多帧索引及排序的问题，学术上能够查询的论文比较少，这里小白菜结合自己的理解以及应用实践总结了小部分的经验，希望对遇到这方面问题的小伙伴有些参考的价值，也欢迎在一线从事CBIR的小伙伴拍砖交流。&lt;/p&gt;
</description>
        <pubDate>Sun, 27 May 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/multi-frames-ranking-problem.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/multi-frames-ranking-problem.html</guid>
      </item>
    
      <item>
        <title>图像检索：图像拷贝检索PHash改进方案</title>
        <description>&lt;p&gt;感知哈希是用来做图像拷贝检索（Copy Detection）最容易操作的一种方法，实际上除了感知哈希、均值哈希，还有很多的从图像本身出发计算出来的图像哈希值，在OpenCV 3.3及其以后的版本中，包含了很多图像哈希的计算方法，具体的可以参考&lt;a href=&quot;https://docs.opencv.org/3.3.1/d4/d93/group__img__hash.html&quot;&gt;The module brings implementations of different image hashing algorithms&lt;/a&gt;，其中各种图像哈希方法对8种不同变化的抗干扰程度，文档中做了一个很好的总结：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://docs.opencv.org/3.3.1/attack_performance.JPG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从图中可以看到，Phash具备较好的对不同变化的抗干扰性，因为在&lt;strong&gt;一般要求不高的图像拷贝检索&lt;/strong&gt;场景中，应用得较多。下面小白菜就PHash的原理（计算步骤）、在使用中存在的问题以及改进方案做一个记录与总结。&lt;/p&gt;

&lt;h3 id=&quot;phash原理&quot;&gt;PHash原理&lt;/h3&gt;

&lt;p&gt;感知哈希（Perceptual Hash, PHash）比均值哈希要稳健，PHash使用DCT将图像由空域转为频域，并对频域的低频成分进行散列化。PHash算法可分为以下几个步骤：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;将图片resize到32*32最好，这样可以简化DCT计算；&lt;/li&gt;
  &lt;li&gt;将彩色图像转为灰度图像；&lt;/li&gt;
  &lt;li&gt;计算DCT，使用32*32的DCT变换；&lt;/li&gt;
  &lt;li&gt;DCT的结果是32*32的矩阵，只要保留左上角的8*8矩阵就可以了，因为这部分呈现了图片中的最低频率；&lt;/li&gt;
  &lt;li&gt;计算DCT的平均值；&lt;/li&gt;
  &lt;li&gt;根据8*8的DCT矩阵来与平均值进行比较，大于平均值的为1，小于的为0。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;注意&lt;/strong&gt;：在保留左上角的8*8矩阵的时候，不一定非得是最左上角，可以往右下移一个或几个像素。下面是采用PHash做拷贝检索结果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://owtbv2q93.bkt.clouddn.com/note/okcase_phash.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，对于上面查询，PHash能够获得很好的拷贝检索准确率，但是PHash除了上面图表所示的对椒盐噪声、旋转几乎不具有抗干扰特性外，还有其他的方面的一些局限。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PHash的局限&lt;/strong&gt;：PHash为了容忍图像的一些形变而只取了图像的低频部分，从而造成了特征捕获不到图像的细节部分，使得对于纯色或者近似纯色图像在做查重的时候并不是很理想。具体地说，由于PHash取的是dct的左上角部分，属于低频成分，也就是关注的是图像的大致外形，没有关注细节，所以对于纯色图片或者近似纯色图片，就没法捕获到它的轮廓细节，导致对纯色图像或者近似纯色图片的查重准确率不高。一种改进的方式是：把这个取的部分往右下移，这样就可以获得图像的轮廓细节，纯色图像或者近似纯色图片查重准确率就会提升，坏处就是轮廓细节取多了，容忍细节变化就小了，导致图像容忍的形变变小，但是这种思路是值得借鉴的。&lt;/p&gt;

&lt;h3 id=&quot;phash特性&quot;&gt;PHash特性&lt;/h3&gt;

&lt;p&gt;PHash对噪声、模糊、jpeg压缩等具备较好的不变性，此外，在实际应用中还有另一类变换也是值得非常关注的，即对图像做水平镜像操作。&lt;strong&gt;PHash对镜像不具备不变性&lt;/strong&gt;，可以通过一个简单的实验予以验证。&lt;/p&gt;

&lt;p&gt;实验过程：测试了两对图片，每一对图片包含图片自身，已经其对应的镜像图片（图片对的大小是一样的，说明为jpeg压缩影响），分别计算图片对之间的距离，一对算出来的距离是35，一对算出来的是33。说明PHash对镜像无召回能力。&lt;/p&gt;

&lt;p&gt;既然谈到了图像的镜像变换，我们不妨对PHash、基于SIFT特征的Fisher Vector以及DL相似特征对镜像变换是否有不变形做一个整理：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;特征&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;形变&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;是否抗干扰&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;备注&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;PHash特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;否&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;见上面说的验证过程&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;FV特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;很弱&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;SIFT对镜像不具备不变性，故FV对镜像召回能力很弱，具体参考论文&lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;amp;arnumber=6336821&quot;&gt;Flip-Invariant SIFT for Copy and Object Detection&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;DL相似特征&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;镜像&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;是&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;由于DL相似特征在训练的时候，数据增强里面包含镜像，DL相似特征对镜像具备旋转特性，检索显示的top@K里面，可以找回镜像的视频，见结果&lt;a href=&quot;http://owtbv2q93.bkt.clouddn.com/note/similarity_flip.png&quot;&gt;镜像检索结果&lt;/a&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;phash改进方案&quot;&gt;PHash改进方案&lt;/h3&gt;

&lt;p&gt;在前面已经提到了PHash对于纯色或近似纯色图像做拷贝检索存在的缺陷，当DCT进行散列化时如果选取的DCT的频率过低，则对纯色或近似纯色图像的拷贝检索存在badcase，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://owtbv2q93.bkt.clouddn.com/note/pureColor_phash.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到，由于查询图像是接近纯色的图片，导致取得DCT的低频只能捕获到图像的大致外观，因而很多接近纯色的图片到排在了前面。对这类case的改进优化，我们知道无论是取低频还是高频部分做散列化都不合适，如果只取高频，则会影响正常图片的召回。因此，比较容易想到的一种改进方式是：对于正常的图片，只需采用低频DCT哈希值做排序；对于纯色或近似纯色图像，先用低频DCT哈希值检索排序，然后再用高频DCT哈希值检索再做重排。这种改进方式的好处是显而易见的，对于每一张图片，只需要额外增加64个比特位的存储空间，并且不用对整个拷贝检索的架构做很大的调整，我们所要做的就是再计算一下高频DCT的哈希值，并且增加一个对纯色或接近纯色的检索服务，就能使PHash在图像拷贝检索上获得较大的精度提升，同时又不至于较大的减少召回。&lt;/p&gt;

&lt;p&gt;对于图像纯色或接近纯色的检索，小白菜以为应该做得轻巧简洁，因为本身PHash做拷贝检索就是一个很轻量的服务，如果图像纯色或接近纯色的检索做的过重，比如用DL对图像纯色与非纯色进行分类，就失去了用PHash做拷贝检索的意义，另外采用还需消耗大量的GPU，因而图像纯色或接近纯色的服务越轻巧越好。下面小白菜提供的一个极轻量的图像纯色或接近纯色的检索方法：&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imGray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;123.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_LOAD_IMAGE_GRAYSCALE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histSize&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;histRange&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calcHist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imGray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;histRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Mat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortIdx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_EVERY_COLUMN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_DESCENDING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_EVERY_COLUMN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CV_SORT_DESCENDING&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;at&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;allFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    
&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxFre&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;secFre&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allFre&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;  
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.51&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;pure image&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;根据灰度测试的结果，在阈值为0.51时，粗略评估对于纯色或接近纯色图像的召回率至少在85%以上，准确率在90%以上，检测速度在10ms左右。这里，对于召回率和准确率的要求是，召回越高越好，对准确率的要求可以相对低一点，因为我们的目的是要改善纯色或接近纯色图像的拷贝检索的准确率，可以小幅牺牲点非纯色图像拷贝检索的召回。&lt;/p&gt;

&lt;h3 id=&quot;改善性能验证&quot;&gt;改善性能验证&lt;/h3&gt;

&lt;p&gt;按上述所提的改进方案重排后，即：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于一个查询，先使用低频分量DCT哈希值进行排序；&lt;/li&gt;
  &lt;li&gt;对查询图像进行纯色或者近似纯色图像检测，如果不是纯色或者近似纯色图像，当前排序结果为最终拷贝检索排序结果；&lt;/li&gt;
  &lt;li&gt;如果是纯色或者近似纯色图像，使用高频DCT哈希值对初排结果进行重新排序，对重排序结果只保留汉明距离只小于等于某一阈值的那些结果，将其作为最终排序结果；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是上面查询图像采用该改进方案重排后的一个结果，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://owtbv2q93.bkt.clouddn.com/note/improved_dct.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面可以看到，经过重排后，对于纯色或者接近纯色图像的拷贝检索，结果有了很大的提升，实际中测试了很多的case，发现都能够获得很好的改善。&lt;/p&gt;

&lt;h3 id=&quot;总结&quot;&gt;总结&lt;/h3&gt;

&lt;p&gt;在本篇文章中，小白菜就PHash的原理、存在的缺陷以及改进的方案做了详细的总结，这个问题的存在以及想到的解决方法并不是凭空产生和获得的，而是实际应用中确确实实会存在这样或那样的问题，需要不断从原理上推敲，然后反复进行实验。当然对PHash的改进应该有非常多，这种改进方案不一定是最好的，但是可以值得借鉴，希望对有需要的同学有所帮助或者启发。&lt;/p&gt;
</description>
        <pubDate>Fri, 16 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/improve-phash-for-copy-detection.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/improve-phash-for-copy-detection.html</guid>
      </item>
    
      <item>
        <title>机器视觉：Asymmetry Problem in Computer Vision</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;自然法则无时不刻不给予着人类以对称性的恩惠，从一片树叶到人类自身，其形态都是对称的。对称性的特性，大大减轻了人类的记忆和认知负担。然而，弱相互作用中互为镜像的物质的运动不对称却暗藏着自然法则对非对称性的偏爱。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在计算机视觉中，对称性是一个很好的先验，如果某一个特定的物体具备对称性的话，通过引入对称性可以提升系统的精度。常见的对称性包括：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;物体本身具备对称性，且这种对称性不容易受大视角变化的影响。主要应用场景为在训练诸如检测模型的时候，可以将这一信息加入到训练样本的扩增上；&lt;/li&gt;
  &lt;li&gt;相似性度量具备对称性。这种对称性常体现在设计的相似性度量准则上，A与B计算出的相似性和B与A得到的相似性是一样。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我们的大脑深受对称性法则的影响，所以对于第一种情况，我们可以非常直观的将这一法则应用到我们的视觉模型训练中，但是对于第二种情况，特别是当相似性度量由非对称性在对称性上转化而来的时候，我们的第一意识却频频出错。计算机视觉中非对称现象和非对称相似性度量如此干扰我们的第一意识，以至于小白菜对这个问题曾做出过若干的思考，下面是小白菜结合自己的一些思考和经验做的总结和整理。&lt;/p&gt;

&lt;h3 id=&quot;局部特征匹配中的非对称问题&quot;&gt;局部特征匹配中的非对称问题&lt;/h3&gt;

&lt;p&gt;在用局部特征进行匹配的时候，比如SIFT，用A图匹配B图和用B图匹配A图，得到的匹配结果是不一样的，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/sift_matching_diff.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这匹配的过程中，所有的度量方式比如最近邻、次近邻、几何校验等都是具备对称性的，所以很容易给我们造成一种错觉就是他们的匹配结果应该是一样的。这里面造成匹配结果不对称的根本因素在于&lt;strong&gt;A图和B图它们的局部特征数目是不相等的&lt;/strong&gt;，比如A有500个局部特征，B有800个局部特征，用500个局部特征去匹配800局部个特征和用800个局部特征去匹配500个局部特征，势必造成匹配的结果不一样。一个鲁棒的匹配算法，应该在用A匹配B和用B匹配A时都能获得比较好的匹配结果，以避免单一结果匹配较好的情形，像下面的匹配算法就不是一种好的匹配算法：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/false_sift_matching.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在设计匹配算法的时候，我们应&lt;strong&gt;避免我们的算法出现单一匹配好的情形，以提高匹配的鲁棒性&lt;/strong&gt;。如需获取上图匹配结果，可以访问&lt;a href=&quot;https://github.com/willard-yuan/covdet.git&quot;&gt;covdet&lt;/a&gt;获取。&lt;/p&gt;

&lt;h3 id=&quot;logo识别中的非对称问题&quot;&gt;Logo识别中的非对称问题&lt;/h3&gt;

&lt;p&gt;Logo的识别问题，可以分为两类:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;基于检测的方式&lt;/li&gt;
  &lt;li&gt;基于检索的方式&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;基于检测方式的缺点在于新的Logo样式来临时，会面临训练样本如何获取以及人工标注的问题，虽然可以自动通过往图片上贴Logo的方式来构造训练样本，但这种方式容易造成过拟合；基于检索的方式的优点在于，它不会面临训练样本获取和标注的问题，能够以较快的速度响应新Logo检测的请求，但是基于Logo检索的方式，面临一个很大的问题就是尺寸不对称性。&lt;/p&gt;

&lt;p&gt;这种尺寸不对称性体现在：对于待检测的图片，Logo所占的区域是很小的，通常区域面积占总体面积比的5%都不到，这样就导致提取的Logo区域的特征（比如局部特征）被非Logo区域的特征给“淹没”掉，如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1029/logo-example.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;上图这个图选取得不是很合适，因为背景比较干净，Logo区域的特征还是其了比较大的作用。对于一般的情况，由于Logo区域的特征被非Logo区域的特征给淹没掉，从而使得在Logo库里检索的时候，在top@K（K取得比较小）里面比较难以检索到相关的Logo，而且即便是做重排，也比较难以将最相关的Logo排到最前面。&lt;/p&gt;

&lt;p&gt;针对这种由于尺寸不对称性造成的有用特征（信号）被无用信号淹没的问题，很难在不对检索精度造成较大影响的前提下找到比较有效的解决办法。如果要剔除无用信号造成的干扰，一种比较好的方法是先进行粗略的检测，这一步不要求检测的准确率很高，只要保证召回率很高即可，然后可以根据定位到的框提取特征，再进行检索以及校验。这种方式由于剔除了非相关区域特征的干扰，所以准确率和召回率通常能够得到较好的保证，唯一不足的是，引入了检测，而检测势必要求对数据进行标注（半自动）。不过总体来说，这仍然是一种非常不错的方法。&lt;/p&gt;

&lt;h3 id=&quot;pq中的非对称距离&quot;&gt;PQ中的非对称距离&lt;/h3&gt;

&lt;p&gt;在此前的文章&lt;a href=&quot;http://yongyuan.name/blog/ann-search.html&quot;&gt;图像检索：再叙ANN Search&lt;/a&gt;中，小白菜曾对PQ做过比较详细的介绍，这里对PQ中非对称距离的计算做一详述。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_search_zpskgugtocx.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，非对称距离计算方式（红框标示）在计算查询向量$x$到库中某一样本$y$之间的距离时，并不需要对查询向量$x$自身进行量化，而是直接计算查询向量$x$到量化了的$y$之间的距离，这种距离计算方式可以确保非距离计算方式以更大的概率保证计算的距离更接近于真实的距离（与对称距离计算方式相比较），而且这种方式比对称距离计算方式在实施的过程中，来得更直接，因为我们不需要对查询向量进行量化，而是直接计算到对应子段之间的距离，然后采用查表的方式获取到查询向量到库中所有样本的距离。这种通过查表的方式，是PQ能够加速距离计算的核心。&lt;/p&gt;

&lt;p&gt;非对称性的应用，在这里展示了它有利的一面，通过将&lt;strong&gt;非对称性延展到相似性度量上，可以进一步缩小量化造成的损失&lt;/strong&gt;。既然谈到了PQ的思想，我们还可以对PQ的改进做一下的延拓。&lt;/p&gt;

&lt;h4 id=&quot;polysemous-codes&quot;&gt;Polysemous Codes&lt;/h4&gt;

&lt;p&gt;PQ的改进版本很多，比如&lt;a href=&quot;kaiminghe.com/publications/pami13opq.pdf&quot;&gt;OPQ&lt;/a&gt;, Jegou等人又在PQ的基础上对PQ计算距离的过程中做了进一步加速，提出了&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;（中译为“一词多义编码”，为何叫Polysemous Codes，容小白菜慢慢道来)。&lt;/p&gt;

&lt;p&gt;前面已经提到，即便是采用查表的方式，仍然还是要查表挨个对库中的每个样本计算到查询向量之间的距离，这种距离能不能转换成汉明距离的计算？正如上图所示的，对于每个向量，我们可以知道它对应的量化索引编码，如果这个量化索引编码本身就是一种汉明编码，那么我们就可以直接用通过计算汉明距离来得到粗排序的结果，然后再对topK的结果计算ADC距离，Polysemous Codes的动机正是如此，实际上Polysemous Codes不仅充当了量化索引编码，还充当了一个快速过滤的作用。&lt;/p&gt;

&lt;p&gt;应该说，对于所有的相似性度量距离，汉明距离计算从效率上来讲，是最快的。在faiss的项目wiki的&lt;a href=&quot;https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)&quot;&gt;re-filtering PQ codes with polysemous codes&lt;/a&gt;有结论：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It is about 6x faster to compare codes with Hamming distances than to use a product quantizer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;也就是计算一次PQ的距离，跟计算一次汉明距离计算相比，会慢6倍左右，说明如果能把PQ编码赋予汉明编码的意义的话，距离的计算会提升6倍，这个提升还是非常巨大的。Polysemous Codes的实现已在Faiss中&lt;a href=&quot;https://github.com/facebookresearch/faiss/blob/master/IndexPQ.h#L67&quot;&gt;Polysemous Codes Implementation&lt;/a&gt;。下图解释一下“Polysemous Codes”的“Polysemous”，即一词多义：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/1202/Polysemous_Codes.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Polysemous codes are compact representations of vectors that can be comparedeither with product quantization (222M distance evaluations per second per core for 8-byte codes) or as binary codes (1.19G distances per second). To obtain this property, we optimize the assignment of quantization indexes to bits such that closest centroidshave a small Hamming distance. 【摘自&lt;a href=&quot;https://arxiv.org/abs/1609.01882&quot;&gt;Polysemous Codes&lt;/a&gt;】&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在标准的PQ编码中，编码仅指代对应的量化索引编码，即由哪个类中心来近似该子段向量，比如上面的左图，4位的二进制编码仅表示类中心编码（编号），除此之外，无任何其他的意义；而在Polysemous Codes中，如右图所示，它不仅能表示类中心的编码（编号），而且它还是一种汉明编码，从这里可以看到，该编码包含了两种特性，我们既可以计算PQ距离，又可以计算汉明距离，因而该编码被称为“Polysemous Codes”是非常妥帖的。由于计算汉明距离更高效，而Polysemous Codes又是一种汉明码，我们可以先用汉明距离进行粗排序，在采用PQ距离进行重排。&lt;/p&gt;

&lt;p&gt;Polysemous Codes所拥有的这两种特性，不能不感叹它是极其优雅美丽的。&lt;/p&gt;

&lt;h3 id=&quot;累计最小最大距离非对称问题&quot;&gt;累计最小（最大）距离非对称问题&lt;/h3&gt;

&lt;p&gt;在度量两个集合的相似性的时候，累计最小（最大）距离是一个比较好用的相似性度量距离。假设两个集合分别为$X = \lbrace x_t, i=1 \dots n \rbrace$和$Y = \lbrace y_t, j=1 \dots m \rbrace$，则$X$和$Y$集合的相似性可以通过累计最小（最大）距离来度量，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
S(X, Y) = \sum_{i=1}^{i=n} \sum_{j=1}^{j=m} d_\min(x_i, y_j)
\end{equation}&lt;/p&gt;

&lt;p&gt;至于$d$选取何种距离，我们可以根据自己的应用场景来定夺，小白菜自己一般喜欢使用余弦相似度，因为该距离的计算最终可以转换成内积。累计最小（最大）距离应用的场景这里可以列举一二：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征构成的两个集合，这里的局部特征不限于传统的局部特征，还可以是CNN构造的局部特征；&lt;/li&gt;
  &lt;li&gt;两个视频序列分别对视频帧提取全局特征，构成的两个视频帧特征集合，使用累计最小（最大）距离我们可以得到两个视频的相似性。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;累计最小（最大）距离通常也是一种非对称性距离，在两个集合样本数量相差得比较大时，这种非对称性表现得非常明显。以上面所说的第二个例子为例，当两个视频由于某种不是很合理的取帧方式，导致两个视频取的帧数目相差比较大时，计算出的$S(X, Y)$和$S(Y, X)$会相差得很大，必然会导致其中的一个得到的相似度比较小。对于这种情形，在此相似性度量方式下，并没有特别好的办法来改善，所以在取帧的时候，尽量使两者的数目相当。&lt;/p&gt;

&lt;p&gt;总体来说，累计最小（最大）距离对于度量两个集合的相似性是个不错的距离度量。下面讲讲对这个距离度量的计算速度优化问题。比如，我们要计算$X$和$Y$两个集合的相似性，并且每个集合有1000个元素，我们是要一个一个遍历计算找最小值然后相加吗？&lt;/p&gt;

&lt;p&gt;显然这种计算方式太慢。前面提到过，在对$d$相似性度量的选取上，小白菜喜欢使用余弦相似性，矩阵乘法使得我们可以避免掉挨个遍历循环，通过矩阵相乘得到$X$和$Y$中各个样本与样本之间的相似性后，我们可以对相似性矩阵进行排序，然后求和相加即可得到$S(X, Y)$，即$X$和$Y$之间的相似性。&lt;/p&gt;

&lt;p&gt;总结一下，在本篇博文中，分别从下面4个方面对计算机视觉中的非对称问题进行了探讨：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部特征匹配中的非对称问题&lt;/li&gt;
  &lt;li&gt;Logo识别中的非对称问题&lt;/li&gt;
  &lt;li&gt;PQ中的非对称距离&lt;/li&gt;
  &lt;li&gt;累计最小（最大）距离非对称问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;中间穿插了对PQ的拓展Polysemous Codes的拓展。后面如遇其他计算机视觉中的非对称性问题，会同步到这里。&lt;/p&gt;
</description>
        <pubDate>Sat, 02 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/asymmetry-problem-in-computer-vision.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/asymmetry-problem-in-computer-vision.html</guid>
      </item>
    
      <item>
        <title>图像检索：Fisher Information Matrix and Fisher Kernel</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://wiki.mbalib.com/wiki/%E7%BD%97%E7%BA%B3%E5%BE%B7%C2%B7%E8%B4%B9%E9%9B%AA&quot;&gt;罗纳德·费雪&lt;/a&gt;（Sir Ronald Aylmer Fisher, FRS，1890.2.17－1962.7.29)，现代统计学与现代演化论的奠基者之一，安德斯·哈尔德称他是“一位几乎独自建立现代统计科学的天才”，理查·道金斯则认为他是“达尔文最伟大的继承者”。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2015/1002/Ronald_Aylmer_Fisher.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;局部特征作为一种强鲁棒性的特征，其与全局特征构成了CV领域图像内容描述的基础。相比于全局特征，局部特征往往在对低层共有模式的表达上可以做到更细的粒度（关于局部和全局在视觉认知上的作用机制，强烈推荐阅读&lt;a href=&quot;https://en.wikipedia.org/wiki/Scale_space&quot;&gt;尺度空间理论&lt;/a&gt;），但同时也引发了新的问题，即&lt;strong&gt;特征处理效率低、存储大等方面的问题&lt;/strong&gt;。因而需要将局部特征经过某种编码方式，最终表示成一种紧凑的全局特征表示。&lt;/p&gt;

&lt;p&gt;Fisher Vector作为连接单向连接局部特征到全局表示的三大特征编码方法之一（另外两种编码方式见&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;图像检索：BoF、VLAD、FV三剑客&lt;/a&gt;），无论是在学术研究领域还是在工业实际应用上，都具有很重要的地位。下面内容是小白菜对Fisher Vector中的Fisher信息矩和Fisher核的数学推导及对应物理意义的总结整理。&lt;/p&gt;

&lt;h2 id=&quot;fisher信息矩和fisher核&quot;&gt;Fisher信息矩和Fisher核&lt;/h2&gt;

&lt;p&gt;在空间$\chi$中，某样本$X$存在$T$个观测量，记为$X = \lbrace x_t, t=1 \dots T \rbrace$。对应到图像上，样本$X$为图像$I$提取到的$T$个$D$维的局部描述子，比如SIFT。设$\mu_\lambda$为概率密度函数，该函数包含有$M$个参数，即$\lambda = [\lambda_1, \dots, \lambda_M]$。根据生成式原理，空间$\chi$中的元素$X$可以由概率密度函数进行建模。在统计学上，分数函数（score funciton）可以由对数似然函数的梯度给出，即：&lt;/p&gt;

&lt;p&gt;\begin{equation}
G^X_\lambda = \nabla log \mu_\lambda(X)
\label{sfun}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式对数函数的梯度，在数学形式上为对数似然函数的一阶偏导，它描述了每一个参数$\lambda_i$对该生成式过程的贡献度，换言之，该分数函数$G^X_\lambda$描述了生成式模型$\mu_\lambda$为了更好的拟合数据$X$，该模型中的参数需要做怎样的调整。又因为$G^X_\lambda \in R^M $是一个维度为$M$维的向量，所以该分数函数的维度仅依赖于$\lambda $中参数的数目$M$, 而于观测样本的数目$T$无关。此外，一般情况下，该分数函数的期望$E[ G^X_\lambda ] = 0 $，这一点对于下面讲到的Fisher信息矩物理意义的得到非常重要。&lt;/p&gt;

&lt;p&gt;根据信息几何理论，含参分布$\Gamma = \lbrace \mu_\lambda, \lambda \in \Lambda \rbrace$可以视为一个黎曼流形$M_\Lambda$，其局部度量方式可以由Fisher信息矩(Fisher Information Matrix, FIM）$F_\lambda \in R^{M \times M}$:&lt;/p&gt;

&lt;p&gt;\begin{equation}
F_{\lambda} = E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ]
\label{fim}
\end{equation}&lt;/p&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是分数函数的二阶矩。在一般条件下很容易证明（注意$E[ G^X_\lambda ] = 0 $）：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
F_{\lambda} &amp;= E_{ x \sim \mu_\lambda } [ G^X_\lambda (G^X_\lambda)^T ] \\\\
            &amp;= E[(G^X_\lambda)^2] \\\\
            &amp;= E[(G^X_\lambda)^2] - E[ G^X_\lambda ]^2 \\\\
            &amp;= Var[G^X_\lambda]
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;从上式可以看到，Fisher信息矩是用来估计最大似然估计（Maximum Likelihood Estimate, MLE）的方程的方差。它直观的表述就是，在独立性假设的条件下，随着收集的观测数据越来越多，这个方差由于是一个相加的形式，因而Fisher信息矩也就变的越来越大，也就表明得到的信息越来越多。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;注：此处引用了&lt;a href=&quot;https://www.zhihu.com/question/26561604&quot;&gt;fisher information 的直观意义是什么?&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;对于两组不同的观察样本$X$和$Y$，Jaakkola和Haussler提出了使用Fisher核来度量它们之间的相似性，其数学表达形式为：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (G^X_\lambda)^T F_\lambda^{-1} G^X_\lambda
\label{fk}
\end{equation}&lt;/p&gt;

&lt;p&gt;又因为$F_{\lambda}$是半正定的，所以其逆矩阵是存在的。使用cholesky分解可以得到$F_\lambda^{-1} = (L_\lambda)^T L_\lambda$，上式可以写成内积的表示形式：&lt;/p&gt;

&lt;p&gt;\begin{equation}
K_FK(X, Y) = (\wp^X_\lambda)^T \wp^Y_\lambda
\label{fk1}
\end{equation}&lt;/p&gt;

&lt;p&gt;其中,
\begin{equation}
\wp^X_\lambda = L_\lambda G^X_\lambda = L_\lambda \nabla log \mu_\lambda(X)
\label{wp_lambda}
\end{equation}&lt;/p&gt;

&lt;p&gt;上式是$L_\lambda$对$G^X_\lambda$的归一化，我们将$\wp^X_\lambda$称为Fisher向量，该Fisher向量$\wp^X_\lambda$等于梯度向量$G^X_\lambda$的维度，又由于$G^X_\lambda$的维度仅与概率密度函数的参数数目$M$有关，所以空间$\chi$中任意样本$X$的$T$个观测量最终都可以表示成一固定维度的向量。通过使用$\wp^X_\lambda$算子，使得非线性核相似性度量问题转化为线性问题。这种变换带来的一个明显的优势是，在分类的时候可以采用更高效的线性分类器。&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Oct 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/fim-fisher-kernel.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/fim-fisher-kernel.html</guid>
      </item>
    
      <item>
        <title>深度学习：Neural Network Layers Understanding</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;我想做又应该做的事，都会做到；我想做却不应做的事，都会戒掉。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;inner-product-layer&quot;&gt;Inner Product Layer&lt;/h2&gt;

&lt;p&gt;Inner Product Layer即全连接层，对于IP层的理解，可以简单的将其视为矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;和矩阵&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;相乘后得到&lt;code class=&quot;highlighter-rouge&quot;&gt;1*M&lt;/code&gt;的维度向量。&lt;/p&gt;

&lt;p&gt;举个简单的例子，比如输入全连接层的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt;维度的数据，假设未知的权重维度为&lt;code class=&quot;highlighter-rouge&quot;&gt;N*M&lt;/code&gt;，假设全连接层的输出为&lt;code class=&quot;highlighter-rouge&quot;&gt;num_ouput = 4096&lt;/code&gt;，为了计算全连接层的输出，全连接层会将输入的数据&lt;code class=&quot;highlighter-rouge&quot;&gt;3*56*56&lt;/code&gt; reshape 成为&lt;code class=&quot;highlighter-rouge&quot;&gt;1*N&lt;/code&gt;的形式，即&lt;code class=&quot;highlighter-rouge&quot;&gt;1x(56x56x3) = 1x9408&lt;/code&gt;，所以：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;N = 9408
M = num_ouput = 4096
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;由此，我们做了一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x4096)&lt;/code&gt;矩阵的乘法。如果&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;的值改变成为100，则做的是一个&lt;code class=&quot;highlighter-rouge&quot;&gt;(1x9408)&lt;/code&gt;矩阵和&lt;code class=&quot;highlighter-rouge&quot;&gt;(9408x100)&lt;/code&gt;矩阵的乘法。&lt;strong&gt;Inner Product layer（常被称为全连接层）将输入视为一个vector，输出也是一个vector（height和width被设为1）&lt;/strong&gt;。下面是IP层的示意图&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/fcgemm_corrected.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;增大&lt;code class=&quot;highlighter-rouge&quot;&gt;num_output&lt;/code&gt;会使得模型需要学习的权重参数增加。IP层一个典型的例子：&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
  # learning rate and decay multipliers for the weights
  param {
    lr_mult: 1
  }
  # learning rate and decay multipliers for the biases
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;有了上面对IP层的理解，对&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/layers/inner_product_layer.cpp&quot;&gt;caffe inner_product_layer.cpp&lt;/a&gt;中Forward的理解就比较自然了。下面是Caffe的IP层在CPU上的实现：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;template&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;typename&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;InnerProductLayer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Forward_cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Blob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;*&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bottom&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mutable_cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transpose_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;?&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;bottom_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias_term_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;caffe_cpu_gemm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CblasNoTrans&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;M_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bias_multiplier_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;blobs_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;top_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面完成矩阵与矩阵相乘的函数是&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&amp;lt;Dtype&amp;gt;&lt;/code&gt;（见&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;)，&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;函数矩阵相乘的具体数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*TransB(B) + beta*C\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上式中&lt;code class=&quot;highlighter-rouge&quot;&gt;TransX&lt;/code&gt;是对&lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;做的一种矩阵变换，比如转置、共轭等，具体是&lt;code class=&quot;highlighter-rouge&quot;&gt;cblas.h&lt;/code&gt;中定义的为枚举类型。在&lt;a href=&quot;https://github.com/BVLC/caffe/blob/master/src/caffe/util/math_functions.cpp&quot;&gt;math_functions.cpp&lt;/a&gt;中，除了定义矩阵与矩阵相乘的&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemm&lt;/code&gt;外，还定义了矩阵与向量的相乘，具体的函数为&lt;code class=&quot;highlighter-rouge&quot;&gt;caffe_cpu_gemv&lt;/code&gt;，其数学表示形式为：&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation}
   C=alpha*TransA(A)*y + beta*y\\\\
\end{equation}&lt;/script&gt;

&lt;p&gt;上面表达式中，&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;是向量，不是标量。&lt;/p&gt;

&lt;h4 id=&quot;参考&quot;&gt;参考&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/&quot;&gt;Why GEMM is at the heart of deep learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35788873/what-is-the-output-of-fully-connected-layer-in-cnn&quot;&gt;What is the output of fully connected layer in CNN?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/seven_first/article/details/47378697&quot;&gt;caffe_cpu_gemm函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/u011762313/article/details/47361571&quot;&gt;Caffe学习：Layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/tutorial/layers.html&quot;&gt;Caffe Layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;gemm&quot;&gt;GEMM&lt;/h2&gt;

&lt;p&gt;在上面的IP层中，我们已经涉及到了GEMM的知识，在这一小节里面，不妨对该知识点做一个延伸。&lt;/p&gt;

&lt;p&gt;GEMM是BLAS (Basic Linear Algebra Subprograms)库的一部分，该库在1979年首次创建。为什么GEMM在深度学习中如此重要呢？我们可以先来看一个图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/gemm_cup_gpu.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://daggerfs.com/&quot;&gt;Yangqing Jia&lt;/a&gt;的&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-93.pdf&quot;&gt;thesis&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上图是采用AlexNet对CNN网络中不同layer GPU和CPU的时间消耗，从更底层的实现可以看到CNN网络的主要时间消耗用在了FC (for fully-connected)和Conv (for convolution)，而FC和Conv在实现上都将其转为了矩阵相乘的形式。举个例子：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/cnn_gemm.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;图片摘自&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面Conv在Caffe中具体实现的时候，会将每一个小的patch拉成一个向量，很多patch构成的向量会构成一个大的矩阵，同样的对于多个卷积核展成一个矩阵形式，从而将图像的卷积转成了矩阵与矩阵的相乘（更形象化的解释参阅&lt;a href=&quot;https://www.zhihu.com/question/28385679/answer/44297845&quot;&gt;在 Caffe 中如何计算卷积？&lt;/a&gt;）。上面可以看到在FC和Conv上消耗的时间GPU占95%，CPU上占89%。因而GEMM的实现高效与否对于整个网络的效率有很大的影响。&lt;/p&gt;

&lt;p&gt;那么什么是GEMM呢？GEMM的全称是GEneral Matrix to Matrix Multiplication，正如其字面意思所表达的，GEMM即表示两个输入矩阵进行相乘，得到一个输出的矩阵。两个矩阵在进行相乘的时候，通常会进行百万次的浮点运算。对于一个典型网络中的某一层，比如一个256 row&lt;em&gt;1152 column的矩阵和一个1152 row&lt;/em&gt;192 column的矩阵，二者相乘57 million (256 x 1152 x 192)的浮点运算。因而，通常我们看到的情形是，一个网络在处理一帧的时候，需要几十亿的FLOPs（Floating-point operations per second，每秒浮点计算）。&lt;/p&gt;

&lt;p&gt;既然知道了GEMM是限制整个网络时间消耗的主要部分，那么我们是不是可以对GEMM做优化调整呢？答案是否定的，GEMM采用Fortran编程语言实现，经过了科学计算编程人员几十年的优化，性能已经极致，所以很难再去进一步的优化，在Nvidia的论文&lt;a href=&quot;http://arxiv.org/pdf/1410.0759.pdf&quot;&gt;cuDNN: Efficient Primitives for Deep Learning&lt;/a&gt;中指出了还存在着其他的一些方法，但是他们最后采用的还是改进的GEMM版本实现。GEMM可匹敌的对手是傅里叶变换，将卷积转为频域的相乘，但是由于在图像的卷积中存在strides，使得傅里叶变换方式很难保持高效。&lt;/p&gt;
</description>
        <pubDate>Tue, 29 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/neural-network-layers-understanding.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/neural-network-layers-understanding.html</guid>
      </item>
    
      <item>
        <title>知行手记：毕业一周年</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;陷于思，简于情&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;六月的校园充满了告别与不舍，不管愿意与否，时间的巨浪一年又一年的将那些停靠在港湾的船儿推向未知的大海。这一切对于时间而言，不过是社会周期性的轮转，但对于每一条驶入大海的船而言，与大地相隔离开的那一刻，却是个体生命进程中最具仪式感的时刻之一——从此，风吹雨打日晒，或行进，或随流。&lt;/p&gt;

&lt;p&gt;如果说生活需要一些仪式感来刺激日复一日的平凡乃至碌碌无为，那么对于行将走完的六月之末，最大的仪式感便是自己已告别学生时代一周年了。所谓记录也是留念，博文余下部分是小白菜对自己半年来（去年下半年的状态&lt;a href=&quot;http://yongyuan.name/blog/year-turned-back.html&quot;&gt;2016年，归零清空&lt;/a&gt;）的&lt;strong&gt;思想&lt;/strong&gt;、&lt;strong&gt;情感&lt;/strong&gt;的一个总结，希望自己力行笃志，勿忘初心。&lt;/p&gt;

&lt;h2 id=&quot;陷于思&quot;&gt;陷于思&lt;/h2&gt;

&lt;p&gt;对于一个内心极其不安分的人而言，日复一复时间的轮转和实体空间中的条条框框都是一种桎梏。对此，小白菜深有同感并以此为然。小白菜深知自己不是一个很安分的人，虽然在现实生活中循规蹈矩，可是主导自我的精神体系却像一匹狂奔在草原的野马，总想着&lt;strong&gt;去浪荡、去经历、去过一种行无定处的生活&lt;/strong&gt;。这样一种精神体系的主导后果就是间歇性情绪低落症，如果在某一段较长时间内，在生活找不到有所期待的事物或者工作中无小有成就的喜感，便会陷入连日的情绪低落症，以至于在心里盘算着要不要make some changes。关于情绪低落甚而陷入沮丧，&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;说:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;我发现一个人生活时避免沮丧的方法就是始终有期待，比如很期待程序算出来的结果，或者很期待尝试的新菜谱的味道，或者是某天的活动，正在追的剧，正在看的书的情节发展，和盘算已久的旅行等等。如果没有期待，就会怀疑自己生活的意义，虽然思考意义没什么不好，但没有积极情绪的反弹，是很难走出来的。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;想必间歇性情绪低落症也不是小白菜独自面临的问题，上面&lt;a href=&quot;https://www.douban.com/people/81194074/&quot;&gt;ErbB4不麋鹿&lt;/a&gt;所说的有所期待，说到底便是一个被讨论了无数次的hope问题。关于hope，小白菜觉得最好的解释，莫过于《肖申克的救赎》中Andy Dufresne所说的：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Hope is a good thing, maybe the best of things and no good thing ever dies.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;在这一点上，史铁生的&lt;a href=&quot;https://book.douban.com/subject/1136988/&quot;&gt;命若琴弦&lt;/a&gt;也有很深刻的探讨。作为一个上班族，在陷入日复一复上班的轮子里后，也难免在某一段时间内走着走着会陷入一定的迷茫，虽然工作上也有很明确的目标，有很多很多的问题要解，可是如果将时间拉得更长一些，比如两年、三年、五年后自己想实现什么样的一个大目标或者想成为什么样的一个人，便也只好用什么努力实现财务自由或者成为特定领域的专家来搪瓷。这样贴近“地气”的目标虽然是好，可是纯碎地靠这种方式去驱动，却也总是陷入自驱力的泥潭，精神状态一直无法摆脱周期性余弦振动的困扰。这一困扰的根本，小白菜以为在于信仰的缺失，只是小白菜却从未曾找到过，以至于小白菜的微信花名一直是“小白菜在寻找”。&lt;/p&gt;

&lt;p&gt;关于信仰，大部分人包括小白菜在内，都很难说是有信仰的。我们可能有一个、两个或是更多的理想，然而这些零星般的理想，却不过是我们生命中的或大或小的一部分，它可能在较长的一段时间内对我们很重要，但是在下一个循环结中，它对于当前的自己而言已经无关痛痒了。信仰却不同，信仰是一个人用尽自己全部的生命进程去追求、捍卫的一项崇高的事业或者精神准则，以至于到了&lt;strong&gt;无法坚持信仰的那一刻，他会选择宝贵生命来表达他最后的忠诚&lt;/strong&gt;，譬如&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E9%87%8B%E5%BB%A3%E5%BE%B7&quot;&gt;Thich Quang Duc&lt;/a&gt;为了捍卫宗教的信仰而选择火的殉道。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/draft/Thich_Quang_Duc.jpg&quot; alt=&quot;Thich Quang Duc&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一个有信仰的人是幸运地，无论在坚持自我、坚持信仰的道路上他都会获得极大的精神满足，就像&lt;a href=&quot;https://book.douban.com/subject/1858513/&quot;&gt;月亮与六便士&lt;/a&gt;中的思特里克兰德一样，他必须画画，就像溺水的人必须挣扎。关于信仰的种种探讨以及如何找到自己的信仰，小白菜自感功力浅薄，所谓历事勤读，答案也许潜藏于生活中，也有可能存在于某本书中，只是需要借由时间去参透。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;无善无恶心之体，有善有恶意之动，知善知恶是良知，为善去恶是格物&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;小白菜愿用一生的时间多读、多思，去理解、践行阳明心学，构建、稳固并完善自己的精神体系。&lt;/p&gt;

&lt;h2 id=&quot;简于情&quot;&gt;简于情&lt;/h2&gt;

&lt;p&gt;如果要用三句简短的话来概括友情、爱情和亲情这半年来的状态，小白菜以为最好的总结莫过如此：&lt;strong&gt;友情逐步沉淀，爱情迟迟未来，亲情依然照旧&lt;/strong&gt;。&lt;/p&gt;

&lt;h3 id=&quot;友情&quot;&gt;友情&lt;/h3&gt;

&lt;p&gt;对于友情，小白菜也越来越赞同并接纳这样一种观点：闲来无事勿相扰。本科四年，读研三年，再到如今工作一年，从曾经的闹哄哄(各种八卦群)到现在各自回归自我平静的生活，与其说是情感的趋于平淡，倒不如说是各自生活和个人精神的沉淀。&lt;strong&gt;那些曾经美好的相处渐渐沉于记忆，而过于平淡的终将忘却，最后内心惦记并念念不忘的可能也就三两知心友人，或志趣相投、或曾经一块儿奋斗&lt;/strong&gt;。他们从不曾随意打扰过小白菜的生活，亦很少有这样的机会让小白菜去帮助过什么(问题他们总能自己搞定)。&lt;/p&gt;

&lt;p&gt;天各一方，岁月相伴，我们探讨技术，分享工作和生活的心得，心心相惜彼此的才能而乐于在合适的机会面前推举对方，小白菜以为，这就是最好最成熟的友情。&lt;/p&gt;

&lt;h3 id=&quot;爱情&quot;&gt;爱情&lt;/h3&gt;

&lt;p&gt;在绝大多数人都会步入的这条路上，爱情对于小白菜而言，还处于故事的起点。虽不愿随随便便找个人共度来日时光，却再也不肯多在上面做些功课。小白菜匠心情怀、锲而不舍、自信满满，但在爱情这条路上，在经历了一次失意后被打趴得止步不前。此后，在爱情这条路上，小白菜是如此的敏感，以致于蜷缩得像一只浑身是刺心也逐渐石化的刺猬，&lt;strong&gt;不轻易靠近人，也不想被无关的人接近&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;所以，从大学到如今工作满一年，时至8年，在历经了一场长达4年之久的单相思后，未曾再有过心动的妹子。也许，小白菜已经失去了喜欢一个人的能力，再或者，小白菜压根就不知道如何去爱一个人。&lt;/p&gt;

&lt;p&gt;过度的执着于爱情的相处理念，未必是一件好事。也只有到了“这把年纪”，才深切体会到爱情成为一种欠债和任务的无奈（苦笑）。蔡永康说：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;15岁觉得游泳难，放弃游泳，到18岁遇到一个你喜欢的人约你去游泳，你只好说“我不会耶”。18岁觉得英文难，放弃英文，28岁出现一个很棒但要会英文的工作，你只好说“我不会耶”。人生前期越嫌麻烦，越懒得学，后来就越可能错过让你动心的人和事，错过新风景。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;再说小白菜对于爱情的理解，在小白菜还小的时候，曾看到过这样一句话：真正的爱，不再于轰轰烈烈，也不再于信誓旦旦，而是在情感的全心投入中，加以责任的相伴，完成平静的相守。十几年后的今天，当小白菜一字一句敲出这段话来时，对于它的理解增加了几许。如果要用影像来表达爱情的释义，小白菜以为&lt;a href=&quot;https://movie.douban.com/subject/2129039/&quot;&gt;飞屋环游记&lt;/a&gt;和&lt;a href=&quot;https://movie.douban.com/subject/5327189/&quot;&gt;伦敦一家人&lt;/a&gt;足以。&lt;/p&gt;

&lt;h3 id=&quot;亲情&quot;&gt;亲情&lt;/h3&gt;

&lt;p&gt;特别喜欢归有光先生的&lt;a href=&quot;https://www.douban.com/group/topic/3096552/&quot;&gt;项脊轩志&lt;/a&gt;，一个人的小阁子，自言自语，哀而不伤，时至如今，仍能诵读一二：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;借书满架，偃仰啸歌，冥然兀坐，万簌有声；而庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱。&lt;br /&gt;
······&lt;br /&gt;
庭有枇杷树，吾妻死之年所手植也，今已亭亭如盖矣。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;“庭阶寂寂，小鸟时来啄食，人至不去。三五之夜，明月半墙，桂影斑驳，风移影动，珊珊可爱”，像流水的生活、似脉脉的亲情，在喧闹的五道口，仍能在内心最柔弱的找到一处沉寂的&lt;a href=&quot;https://book.douban.com/subject/1865089/&quot;&gt;瓦尔登湖&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;小白菜特别希望老妈和老爸对生活充满了那么一点点的情调，譬如养一盆绿萝，在收到老姐送给的康乃馨时不是再磕磕叨叨又说乱花钱了······这一切希冀的小期待对于那些经历过大锅饭的父辈们来说，实在是苛求。&lt;/p&gt;

</description>
        <pubDate>Wed, 02 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/graduate-after-one-year.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/graduate-after-one-year.html</guid>
      </item>
    
      <item>
        <title>图像检索：layer选择与fine-tuning性能提升验证</title>
        <description>&lt;blockquote&gt;
  &lt;p&gt;这个世界上肯定有另一个我，做着我不敢做的事，过着我想过的生活。一个人逛街，一个人吃饭，一个人旅行，一个人做很多事。极致的幸福，存在于孤独的深海。在这样日复一日的生活里，我逐渐和自己达成和解。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;作为迁移学习的一种，finetune能够将general的特征转变为special的特征，从而使得转移后的特征能够更好的适应目标任务，而图像检索最根本的问题，仍在于如何在目标任务上获得更好的特征表达(共性与可区分性)。一种很自然的方式便是在特定的检索任务上，我们对imageNet学得的general的特征通过finetune的方式，使得表达的特征能够更好的适应我们的检索任务。在&lt;a href=&quot;https://arxiv.org/abs/1610.07940&quot;&gt;End-to-end Learning of Deep Visual Representations for Image Retrieval&lt;/a&gt;和&lt;a href=&quot;https://www.computer.org/csdl/trans/tp/preprint/07867860.pdf&quot;&gt;
Collaborative Index Embedding for Image Retrieval&lt;/a&gt;中已经很清楚的指出，通过基本的classification loss的finetune的方式，能够较大幅度的提高检索的mAP。因此，在本篇博文中，小白菜针对检索，主要整理了下面四个方面的内容：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN网络中哪一层最适合于做图像检索&lt;/li&gt;
  &lt;li&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/li&gt;
  &lt;li&gt;抽取网络任意层的特征&lt;/li&gt;
  &lt;li&gt;数据增强(Data Augmentation)&lt;/li&gt;
  &lt;li&gt;VGGNet16网络模型fine-tuning实践&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在采用深度学习做检索的时候，上面四方面的问题和知识基本都回避不了，因此，小白菜以为，掌握这四方面的内容显得非常有必要。&lt;/p&gt;

&lt;h2 id=&quot;特征表达layer选择&quot;&gt;特征表达layer选择&lt;/h2&gt;

&lt;p&gt;在AlexNet和VGGNet提出伊始，对于检索任务，小白菜相信，在使用pre-trained模型抽取特征的时候，我们最最自然想到的方式是抽取全连接层中的倒数第一层或者倒数第二层的特征，这里说的倒数第一层或者倒数第二层并没有具体指明是哪一层（fcx、fcx_relux、fcx_dropx），以VggNet16网络为例，全连接层包含两层，fc6和fc7，因此我们很自然想到的网络层有fc6、fc6_relu6、fc7、fc7_relu7甚至fc6_drop6和fc7_drop7（后面会说明fc6_drop6和fc6_relu6是一样的，以及fc7_drop7和fc7_relu7也是一样的），所以即便对于我们最最自然最最容易想到的方式，也面临layer的选择问题。为此，我们以VGGNet16网络为例，来分析CNN网络的语义层(全连接层)选择不同层作为特征做object retrieval的mAP的影响。&lt;/p&gt;

&lt;p&gt;小白菜选取fc6、fc6_relu6、fc7、fc7_relu7这四层语义层的特征，在&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/&quot;&gt;Oxford Building&lt;/a&gt;上进行实验，评价指标采用mAP，mAP的计算采用Oxford Building提供的计算mAP代码&lt;a href=&quot;http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp&quot;&gt;compute_ap.cpp&lt;/a&gt;，下表是fc6、fc6_relu6、fc7、fc7_relu7对应的mAP。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(128维)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(4096维)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(4096维, 未做PCA)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc7_relu7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;44.72%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1.11%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41.08%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.03%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19.67%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;41.18%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc6_relu6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43.62%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;23.0%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;43.34%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fc6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.9%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;19.47%&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;44.78%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;从上表可以看到，直接采用pre-trained模型抽取语义层的特征，在Oxford Building上取得的结果在45%左右，同时我们还可以看出，选取fc6、fc6_relu6、fc7、fc7_relu7对结果的影响并不大。这个结果只能说非常的一般，在基于pre-trained模型做object retrieval的方法中，比如&lt;a href=&quot;https://arxiv.org/abs/1512.04065&quot;&gt;Cross-dimensional Weighting for Aggregated Deep Convolutional Features&lt;/a&gt;、&lt;a href=&quot;https://arxiv.org/abs/1511.05879&quot;&gt;Particular object retrieval with integral max-pooling of CNN activations&lt;/a&gt;以及&lt;a href=&quot;https://arxiv.org/abs/1611.01640&quot;&gt;What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?&lt;/a&gt;指出，选用上层的语义层其实是不利于object retrieval，因为上层的语义层丢失了object的空间信息，并且从实验的角度说明了&lt;strong&gt;选取中间层的特征&lt;/strong&gt;更利于object retrieval。&lt;/p&gt;

&lt;p&gt;实际上，在选取中间层来表达特征的过程中，我们可以去掉全连接层，从而使得我们可以摆脱掉输入图像尺寸约束(比如224*224)的约束，而保持原图大小的输入。通常，图像分辨率越大，对于分类、检测等图像任务是越有利的。因而，从这一方面讲，&lt;strong&gt;选取上层的全连接层作为特征，并不利于我们的object retrieval任务&lt;/strong&gt;。一种可能的猜想是，上层全连接层的语义特征，应该更适合做全局的相似。&lt;/p&gt;

&lt;p&gt;虽然中间层更适合于做object retrieval，但是在选用中间层的feature map作为raw feature的时候，我们面临的一个主要问题是：如何将3d的tensor转成一个有效的向量特征表示？下面小白菜主要针对这一主要问题总结几种典型的特征表示方法，以及对中间层特征选择做一些探讨与实验。&lt;/p&gt;

&lt;h2 id=&quot;基于pre-trained模型做object-retrieval几种典型的特征表示&quot;&gt;基于pre-trained模型做Object Retrieval几种典型的特征表示&lt;/h2&gt;

&lt;h3 id=&quot;sum-pooling&quot;&gt;SUM pooling&lt;/h3&gt;

&lt;p&gt;基于SUM pooling的中层特征表示方法，指的是针对中间层的任意一个channel（比如VGGNet16, pool5有512个channel），将该channel的feature map的所有像素值求和，这样每一个channel得到一个实数值，N个channel最终会得到一个长度为N的向量，该向量即为SUM pooling的结果。&lt;/p&gt;

&lt;h3 id=&quot;ave-pooling&quot;&gt;AVE pooling&lt;/h3&gt;

&lt;p&gt;AVE pooling就是average pooling，本质上它跟SUM pooling是一样的，只不过是将像素值求和后还除以了feature map的尺寸。小白菜以为，&lt;strong&gt;AVE pooling可以带来一定意义上的平滑，可以减小图像尺寸变化的干扰&lt;/strong&gt;。设想一张224&lt;em&gt;224的图像，将其resize到448&lt;/em&gt;448后，分别采用SUM pooling和AVE pooling对这两张图像提取特征，我们猜测的结果是，SUM pooling计算出来的余弦相似度相比于AVE pooling算出来的应该更小，也就是AVE pooling应该稍微优于SUM pooling一些。&lt;/p&gt;

&lt;h3 id=&quot;max-pooling&quot;&gt;MAX pooling&lt;/h3&gt;

&lt;p&gt;MAX pooling指的是对于每一个channel（假设有N个channel），将该channel的feature map的像素值选取其中最大值作为该channel的代表，从而得到一个N维向量表示。小白菜在&lt;a href=&quot;https://github.com/willard-yuan/flask-keras-cnn-image-retrieval/blob/master/extract_cnn_vgg16_keras.py&quot;&gt;flask-keras-cnn-image-retrieval&lt;/a&gt;中采用的正是MAX pooling的方式。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/max_pooling_zpsglehm2jv.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://www.slideshare.net/xavigiro/contentbased-image-retrieval-d2l6-insightdcu-machine-learning-workshop-2017&quot;&gt;Day 2 Lecture 6 Content-based Image Retrieval&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;上面所总结的SUM pooling、AVE pooling以及MAX pooling，这三种pooling方式，在小白菜做过的实验中，MAX pooling要稍微优于SUM pooling、AVE pooling。不过这三种方式的pooling对于object retrieval的提升仍然有限。&lt;/p&gt;

&lt;h3 id=&quot;mop-pooling&quot;&gt;MOP pooling&lt;/h3&gt;

&lt;p&gt;MOP Pooling源自&lt;a href=&quot;https://arxiv.org/abs/1403.1840&quot;&gt;Multi-scale Orderless Pooling of Deep Convolutional Activation Features&lt;/a&gt;这篇文章，一作是Yunchao Gong，此前在搞&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;哈希&lt;/a&gt;的时候，读过他的一些论文，其中比较都代表性的论文是ITQ，小白菜还专门写过一篇笔记&lt;a href=&quot;http://yongyuan.name/blog/itq-hashing.html&quot;&gt;论文阅读：Iterative Quantization迭代量化&lt;/a&gt;。MOP pooling的基本思想是多尺度与VLAD(VLAD原理可以参考小白菜之前写的博文&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;图像检索：BoF、VLAD、FV三剑客&lt;/a&gt;)，其具体的pooling步骤如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/mop_cnn_zpstvgo29kk.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://arxiv.org/abs/1403.1840&quot;&gt;Multi-scale Orderless Pooling of Deep Convolutional Activation Features&lt;/a&gt;&lt;br /&gt;
Overview  of  multi-scale  orderless  pooling  for  CNN  activations  (MOP-CNN). Our proposed feature is a concatenation of the feature vectors from three levels: (a)Level 1, corresponding to the 4096-dimensional CNN activation for the entire 256&lt;em&gt;256image; (b) Level 2, formed by extracting activations from 128&lt;/em&gt;128 patches and VLADpooling them with a codebook of 100 centers; (c) Level 3, formed in the same way aslevel 2 but with 64*64 patches.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;具体地，在L=1的尺度下，也就是全图，直接resize到256&lt;em&gt;256的大小，然后送进网络，得到第七层全连接层4096维的特征；在L=2时，使用128&lt;/em&gt;128(步长为32)的窗口进行滑窗，由于网络的图像输入最小尺寸是256&lt;em&gt;256，所以作者将其上采样到256&lt;/em&gt;256，这样可以得到很多的局部特征，然后对其进行VLAD编码，其中聚类中心设置为100，4096维的特征降到了500维，这样便得到了50000维的特征，然后将这50000维的特征再降维得到4096维的特征；L=3的处理过程与L=2的处理过程一样，只不过窗口的大小编程了64*64的大小。&lt;/p&gt;

&lt;p&gt;作者通过实验论证了MOP pooling这种方式得到的特征一定的不变性。基于这种MOP pooling小白菜并没有做过具体的实验，所以实验效果只能参考论文本身了。&lt;/p&gt;

&lt;h3 id=&quot;crow-pooling&quot;&gt;CROW pooling&lt;/h3&gt;

&lt;p&gt;对于Object Retrieval，在使用CNN提取特征的时候，我们所希望的是在有物体的区域进行特征提取，就像提取局部特征比如SIFT特征构&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoF-VLAD-FV.html&quot;&gt;BoW、VLAD、FV向量&lt;/a&gt;的时候，可以采用MSER、Saliency等手段将SIFT特征限制在有物体的区域。同样基于这样一种思路，在采用CNN做Object Retrieval的时候，我们有两种方式来更细化Object Retrieval的特征：一种是先做物体检测然后在检测到的物体区域里面提取CNN特征；另一种方式是我们通过某种权重自适应的方式，加大有物体区域的权重，而减小非物体区域的权重。CROW pooling ( &lt;a href=&quot;https://arxiv.org/abs/1512.04065&quot;&gt;Cross-dimensional Weighting for Aggregated Deep Convolutional Features&lt;/a&gt; )即是采用的后一种方法，通过构建Spatial权重和Channel权重，CROW pooling能够在&lt;strong&gt;一定程度上&lt;/strong&gt;加大感兴趣区域的权重，降低非物体区域的权重。其具体的特征表示构建过程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/crow_zpsaejbmsln.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其核心的过程是Spatial Weight和Channel Weight两个权重。Spatial Weight具体在计算的时候，是直接对每个channel的feature map求和相加，这个Spatial Weight其实可以理解为saliency map。我们知道，通过卷积滤波，响应强的地方一般都是物体的边缘等，因而将多个通道相加求和后，那些非零且响应大的区域，也一般都是物体所在的区域，因而我们可以将它作为feature map的权重。Channel Weight借用了IDF权重的思想，即对于一些高频的单词，比如“the”，这类词出现的频率非常大，但是它对于信息的表达其实是没多大用处的，也就是它包含的信息量太少了，因此在BoW模型中，这类停用词需要降低它们的权重。借用到Channel Weight的计算过程中，我们可以想象这样一种情况，比如某一个channel，其feature map每个像素值都是非零的，且都比较大，从视觉上看上去，白色区域占据了整个feature map，我们可以想到，这个channel的feature map是不利于我们去定位物体的区域的，因此我们需要降低这个channel的权重，而对于白色区域占feature map面积很小的channel，我们认为它对于定位物体包含有很大的信息，因此应该加大这种channel的权重。而这一现象跟IDF的思想特别吻合，所以作者采用了IDF这一权重定义了Channel Weight。&lt;/p&gt;

&lt;p&gt;总体来说，这个Spatial Weight和Channel Weight的设计还是非常巧妙的，不过这样一种pooling的方式只能在一定程度上契合感兴趣区域，我们可以看一下Spatial Weight*Channel Weight的热力图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/sp_example_zps4ntos3ok.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上面可以看到，权重大的部分主要在塔尖部分，这一部分可以认为是discriminate区域，当然我们还可以看到，在图像的其他区域，还有一些比较大的权重分布，这些区域是我们不想要的。当然，从小白菜可视化了一些其他的图片来看，这种crow pooling方式并不总是成功的，也存在着一些图片，其权重大的区域并不是图像中物体的主体。不过，从千万级图库上跑出来的结果来看，crow pooling这种方式还是可以取得不错的效果。&lt;/p&gt;

&lt;h3 id=&quot;rmac-pooling&quot;&gt;RMAC pooling&lt;/h3&gt;

&lt;p&gt;RMAC pooling的池化方式源自于&lt;a href=&quot;https://arxiv.org/pdf/1511.05879&quot;&gt;Particular object retrieval with integral max-pooling of CNN activations&lt;/a&gt;，三作是Hervé Jégou(和Matthijs Douze是好基友)。在这篇文章中，作者提出来了一种RMAC pooling的池化方式，其主要的思想还是跟上面讲过的MOP pooling类似，采用的是一种变窗口的方式进行滑窗，只不过在滑窗的时候，不是在图像上进行滑窗，而是在feature map上进行的(极大的加快了特征提取速度)，此外在合并local特征的时候，MOP pooling采用的是VLAD的方式进行合并的，而RMAC pooling则处理得更简单(简单并不代表效果不好)，直接将local特征相加得到最终的global特征。其具体的滑窗方式如下图所示:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/rmac_pooling_zpsigvxjjud.JPEG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;from &lt;a href=&quot;https://www.slideshare.net/xavigiro/contentbased-image-retrieval-d2l6-insightdcu-machine-learning-workshop-2017&quot;&gt;Day 2 Lecture 6 Content-based Image Retrieval&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;图中示意的是三种窗口大小，图中‘x’代表的是窗口的中心，对于每一个窗口的feature map，论文中采用的是MAX pooling的方式，在L=3时，也就是采用图中所示的三种窗口大小，我们可以得到20个local特征，此外，我们对整个fature map做一次MAX pooling会得到一个global特征，这样对于一幅图像，我们可以得到21个local特征(如果把得到的global特征也视为local的话)，这21个local特征直接相加求和，即得到最终全局的global特征。论文中作者对比了滑动窗口数量对mAP的影响，从L=1到L=3，mAP是逐步提升的，但是在L=4时，mAP不再提升了。实际上RMAC pooling中设计的窗口的作用是定位物体位置的(CROW pooling通过权重图定位物体位置)。如上图所示，在窗口与窗口之间，都是一定的overlap，而最终在构成global特征的时候，是采用求和相加的方式，因此可以看到，那些重叠的区域我们可以认为是给予了较大的权重。&lt;/p&gt;

&lt;p&gt;上面说到的20个local特征和1个global特征，采用的是直接合并相加的方式，当然我们还可以把这20个local特征相加后再跟剩下的那一个global特征串接起来。实际实验的时候，发现串接起来的方式比前一种方式有2%-3%的提升。在规模100万的图库上测试，RMAC pooling能够取得不错的效果，跟Crow pooling相比，两者差别不大。&lt;/p&gt;

&lt;p&gt;上面总结了6中不同的pooling方式，当然还有很多的pooling方式没涵盖不到，在实际应用的时候，小白菜比较推荐采用RMAC pooling和CROW pooling的方式，主要是这两种pooling方式效果比较好，计算复杂度也比较低。&lt;/p&gt;

&lt;h2 id=&quot;抽取网络任意层的特征&quot;&gt;抽取网络任意层的特征&lt;/h2&gt;

&lt;p&gt;在上面一节中，我们频繁的对网络的不同层进行特征的抽取，并且我们还提到fc6_dropx和fc6_relux是一样的（比如fc7_drop7和fc7_relu7是一样的），这一节主要讲述使用Caffe抽取网络任意一层的特征，并从实验的角度验证fc6_dropx和fc6_relux是一样的这样一个结论。&lt;/p&gt;

&lt;p&gt;为了掌握Caffe中网络任意一层的特征提取，不妨以一个小的题目来说明此问题。题目内容为：给定VGGNet16网络，抽取fc7、fc7_relu7以及fc7_drop7层的特征。&lt;br /&gt;
求解过程：VggNet16中&lt;a href=&quot;https://gist.githubusercontent.com/ksimonyan/211839e770f7b538e2d8/raw/0067c9b32f60362c74f4c445a080beed06b07eb3/VGG_ILSVRC_16_layers_deploy.prototxt&quot;&gt;deploy.txt&lt;/a&gt;中跟fc7相关的层如下：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;relu7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: RELU
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;drop7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DROPOUT
  dropout_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    dropout_ratio: 0.5
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;如果使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7'].data[0]&lt;/code&gt;，我们抽取的特征是fc7层的特征，也就是上面：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;这一层的特征，仿照抽取fc7特征抽取的代码，我们很自然的想到抽取relu7的特征为&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['relu7'].data[0]&lt;/code&gt;和drop7的特征为&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['drop7'].data[0]&lt;/code&gt;，但是在运行的时候提示不存在&lt;code class=&quot;highlighter-rouge&quot;&gt;relu7&lt;/code&gt;层和&lt;code class=&quot;highlighter-rouge&quot;&gt;drop7&lt;/code&gt;层，原因是：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;To elaborate a bit further: The layers drop7 and relu7 have the same blobs as top and bottom, respectively, and as such the blobs’ values are manipulated directly by the layers. The advantage is saving a bit of memory, with the drawback of not being able to read out the state the values had before being fed through these two layers. It is simply not saved anywhere. If you want to save it, you can just create another two blobs and re-wire the layers a bit.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;摘自&lt;a href=&quot;https://groups.google.com/forum/#!topic/caffe-users/766VK11Cnwo&quot;&gt;Extracting ‘relu’ and ‘drop’ blobs with pycaffe&lt;/a&gt;，因而，为了能够提取relu7和drop7的特征，我们需要将上面的配置文件做些更改，主要是将layers里面的字段换下名字(在finetune模型的时候，我们也需要做类似的更改字段的操作)，这里小白菜改成了：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc6&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 4096
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_relu7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_relu7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: RELU
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_drop7&quot;&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc7_drop7&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DROPOUT
  dropout_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    dropout_ratio: 0.5
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;经过这样的修改后，我们使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7_relu7'].data[0]&lt;/code&gt;即可抽取到relu7的特征，使用&lt;code class=&quot;highlighter-rouge&quot;&gt;net.blobs['fc7_drop7'].data[0]&lt;/code&gt;可抽取到drop7的特征。&lt;/p&gt;

&lt;h2 id=&quot;数据增强&quot;&gt;数据增强&lt;/h2&gt;

&lt;p&gt;有了上面&lt;strong&gt;CNN网络中哪一层最适合于做图像检索&lt;/strong&gt;、&lt;strong&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/strong&gt;以及&lt;strong&gt;抽取网络任意层的特征&lt;/strong&gt;三方面的知识储备后，在具体fine-tuning网络进行图像检索实验前，还有一节很重要(虽然我们都很熟悉)内容，即数据增强(Data Augmentation)。数据增强作用有二：一是均衡样本，比如某些类别只有几张图片，而有的类别有上千张，如果不做均衡，分类的时候计算的分类准确率会向样本充足的类别漂移；二是提高网络对于样本旋转、缩放、模糊等的鲁棒性，提高分类精度。在实际工作中，我们拿到了图像数据样本对采用深度学习模型而言，经常是不充足且不均衡的，所以这一步数据的前置处理是非常重要的。&lt;/p&gt;

&lt;p&gt;在正式开始数据增强之前，对&lt;strong&gt;图片进行异常检测是非常重要的&lt;/strong&gt;，其具体的异常表现在图片内容缺失、图片不可读取或者可以读取但数据出现莫名的问题，举个例子，比如通过爬虫爬取的图片，可能上半部分是正常的，下半分缺失一片灰色。因此，如果你不能确保你训练的图片数据都是可正常读取的时候，最好对图片做异常检测。假设你的训练图片具有如下层级目录：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_diff tree -L 2
.
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;下面是小白菜参考网上资料写的图片异常检测代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;glob&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;check_pic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;imgs_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata/*/*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#img = Image.open(img_path)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#if img.verify() is not None or img is None:&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;#    print img_path&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;IOError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;SystemError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ERROR: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;通过上面的图片异常检测，我们可以找到那些不可读取或者读取有问题的图片找出来，这样在我们使用Caffe将图片转为LMDB数据存储的时候，不会出现图片读取有问题的异常。在图片异常检测完成后，便可以继续后面的数据增强了。&lt;/p&gt;

&lt;p&gt;在小白菜调研的数据增强工具中，小白菜以为，最好用的还是&lt;a href=&quot;preprocessing&quot;&gt;Keras&lt;/a&gt;中的数据增强。Keras数据增强部分包含在&lt;a href=&quot;https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py&quot;&gt;image.py&lt;/a&gt;，通过类&lt;code class=&quot;highlighter-rouge&quot;&gt;ImageDataGenerator&lt;/code&gt;可以看到Keras包含了对图片的不同处理，下面是小白菜基于Keras写的数据增强脚本，假设你的图像数据目录结构具有如下结构：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_dataset tree -L 2
.
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
    ....
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;数据增强的脚本如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#-*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Author: yuanyong.name&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.preprocessing.image&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataGenerator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;array_to_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_to_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_img&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata'&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;datagen&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataGenerator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;rotation_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;width_shift_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;height_shift_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shear_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;zoom_range&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;horizontal_flip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fill_mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'nearest'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;walk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;#print &quot;Aug: %s&quot; % img_path&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a PIL image, please replace to your own file path&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_to_array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a Numpy array with shape (3, 150, 150)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# this is a Numpy array with shape (1, 3, 150, 150)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datagen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_to_dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_prefix&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'aug'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_perAug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# otherwise the generator would loop indefinitely&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt;
    
&lt;span class=&quot;c&quot;&gt;# delete extra aug images&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_path_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sub_dir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basenames&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;startswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'aug_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_del&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_wanted&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;aug_imgs_del&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aug_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_del&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aug_imgs_del&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub_dir_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_basename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;target_path_dir&lt;/code&gt;是设置你训练数据集目录的，&lt;code class=&quot;highlighter-rouge&quot;&gt;num_wanted&lt;/code&gt;是设置每一个类你想要多少个样本(包括原始样本在内)，比如在&lt;a href=&quot;https://arxiv.org/abs/1404.1777&quot;&gt;Neural Codes for Image Retrieval&lt;/a&gt;中的数据集landmark数据集，里面的类别少的样本只有9个，多的类别样本有上千个，样本部分极其不均匀，上面代码中，小白菜设置得是&lt;code class=&quot;highlighter-rouge&quot;&gt;num_wanted=800&lt;/code&gt;，即让每一个类别的样本数据控制在800左右(多出来的类别样本不会删除，比如有的类别可能原始样本就有1000张，那么不会对该类做数据增强)。&lt;/p&gt;

&lt;p&gt;在这一节，小白菜总结了为什么要数据增强，或者说数据增强有什么好处，然后提供了两个脚本：图片异常检测脚本&lt;a href=&quot;https://github.com/willard-yuan/util-scripts/blob/master/py/dl/check_image.py&quot;&gt;check_image.py&lt;/a&gt;和数据增强脚本&lt;a href=&quot;https://github.com/willard-yuan/util-scripts/blob/master/py/dl/keras_imgAug.py&quot;&gt;keras_imgAug.py&lt;/a&gt;。有了前面三部分的知识和本节数据前置处理的实践，我们终于可以进行fine-tuning了。&lt;/p&gt;

&lt;h2 id=&quot;vggnet16网络模型fine-tuning实践&quot;&gt;VGGNet16网络模型fine-tuning实践&lt;/h2&gt;

&lt;p&gt;在实际中，用CNN做分类任务的时候，一般我们总是用在ImageNet上预训练好的模型去初始化待训练模型的权重，也就是不是train from scratch，主要原因有二：一是在实际中很难获取到大量样本(即便是做了数据增强)；二是加快模型训练的速度。因而，针对检索这个任务，我们也采用fine-tuning的方式，让在ImageNet上预训练的模型迁移到我们自己的特定的数据集上，从而提升特征在检索任务上的表达能力。下面小白菜以fine-tuning Neural Codes提供的数据集为例，比较详实的总结一个完整的fine-tuning过程。&lt;/p&gt;

&lt;p&gt;在fine-tuning之前，我们先追问一个简单的问题和介绍一下Neural Codes提供的landmark数据集。追问的这个问题很简单：为什么几乎所有的做检索的论文中，使用的都是AlexNet、VGGNet16（偶尔会见到一两篇使用ResNet101）网络模型？难道做研究的只是关注方法，使用AlexNet、VGGNet、ResNet或者Inception系列只是替换一下模型而已？小白菜曾也有过这样的疑问，但是对这些网络测试下来，发觉VGGNet在做基于预训练模型特征再表达里面效果是最好的，对于同一个方法，使用ResNet或Inception系列，其mAP反而没有VGGNet的高。至于为什么会这样，小白菜也没有想明白(如果有小伙伴知道，请告知)，我们就暂且把它当做一条经验。&lt;/p&gt;

&lt;p&gt;我们再对Neural Codes论文里提供的landmark数据集做一个简单的介绍。该数据集共有680类，有的类别样本数据至于几个，多则上千，样本分布极其不均匀。不过这不是问题，通过第4节介绍的数据增强和提供的脚本，我们可以将每个类别的样本数目控制在800左右。同时，我们可以使用下面脚本将每个类别所在目录的文件夹名字命名为数字：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata'&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 该目录下有很多子文件夹，每个子文件夹是一个类别&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;walk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;得到清洗后的数据后，下面分步骤详解使用VGGNet16来fine-tuning的过程。&lt;/p&gt;

&lt;h3 id=&quot;切分数据集为train和val&quot;&gt;切分数据集为train和val&lt;/h3&gt;

&lt;p&gt;经过上面重命名后的数据集文件夹目录具有如下形式：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;➜  imgs_dataset tree -L 2
.
├── 0
│   ├── 1227150149_1.jpg
│   ├── 1612549977_1.jpg
│   ├── 1764084098_1.jpg
│   ├── 1764084288_1.jpg
│   └── 1764085346_1.jpg
│   ....
└── 1
    ├── 1227150149_1.jpg
    ├── 1612549977_1.jpg
    ├── 1764084098_1.jpg
    ├── 1764084288_1.jpg
    └── 1764085346_1.jpg
    ....
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;我们需要将数据集划分为train数据集和val数据集，注意val数据集并不单纯只是在训练的时候测试一下分类的准确率。为了方便划分数据集，小白菜写了如下的脚本，可以很方便的将数据集划分为train数据集和val数据集：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;glob&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;argparse&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;random&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;classes_path&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/raid/yuanyong/neuralcode/ncdata/*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 该目录下有很多子文件夹，每个子文件夹train_samples = []&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;imgs_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classes_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'//*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# 训练集占60%&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1024&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;val_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgs_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;' '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;imgs_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_imgs&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;val_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lmdb/train.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lmdb/val.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'w'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;f_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;运行上面脚本，会在lmdb目录下(事先需要建立lmdb目录)生成两个文本文件，分别为&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;，对于为训练数据集和验证数据集。&lt;/p&gt;

&lt;h3 id=&quot;图片转成lmdb存储&quot;&gt;图片转成lmdb存储&lt;/h3&gt;

&lt;p&gt;为了提高图片的读取效率，Caffe将图片转成lmdb进行存储，在上面得到&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;后，我们需要借助caffe的&lt;code class=&quot;highlighter-rouge&quot;&gt;convert_imageset&lt;/code&gt;工具将图片resize到某一固定的尺寸，同时转成为lmdb格式存储。下面是小白菜平时使用的完成该任务的一个简单脚本&lt;code class=&quot;highlighter-rouge&quot;&gt;crop.sh&lt;/code&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;/home/yuanyong/caffe/build/tools/convert_imageset &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         --resize_height 256 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         --resize_width 256 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         / &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         lmdb/train.txt &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
         lmdb/train_lmdb
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;运行两次，分别对应于&lt;code class=&quot;highlighter-rouge&quot;&gt;train.txt&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val.txt&lt;/code&gt;。运行完后，会在lmdb目录下生成&lt;code class=&quot;highlighter-rouge&quot;&gt;train_lmdb&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;val_lmdb&lt;/code&gt;两目录，为了校验转成lmdb存储是否成功，我们最好分别进入这两个目录下看看文件的大小以做简单的验证。&lt;/p&gt;

&lt;h3 id=&quot;生成均值文件&quot;&gt;生成均值文件&lt;/h3&gt;

&lt;p&gt;对于得到的&lt;code class=&quot;highlighter-rouge&quot;&gt;train_lmdb&lt;/code&gt;，我们在其上计算均值。具体地，使用Caffe的&lt;code class=&quot;highlighter-rouge&quot;&gt;compute_image_mean&lt;/code&gt;工具：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/build/tools/compute_image_mean lmdb/train_lmdb lmdb/mean.binaryproto
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;在lmdb目录下即可得到均值文件&lt;code class=&quot;highlighter-rouge&quot;&gt;mean.binaryproto&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;修改train_valprototxt和solverprototxt&quot;&gt;修改train_val.prototxt和solver.prototxt&lt;/h3&gt;

&lt;p&gt;针对VGGNET16网络，在fine-tuning的时候，我们通常将最后的分类层的学习率设置得比前面网络层的要大，一般10倍左右。当然，我们可以结合自己的需要，可以将前面层的学习率都置为0，这样网络在fine-tuning的时候，只调整最后一层分类层的权重；在或者我们分两个阶段去做fine-tuning，第一阶段只fine-tuning最后的分类层，第二阶段正常的fine-tuning所有的层(包含最后的分类层)。同时，我们还需要对最后一层分类层重新换个名字，并且对应的分类输出类别也需要根据自己数据集的分类类别数目做调整，下面小白菜给出自己在fine-tuning Neural Codes的landmark数据集上train_val.prototxt的前面输入部分和后面分类的部分：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;name: &lt;span class=&quot;s2&quot;&gt;&quot;VGG_ILSVRC_16_layers&quot;&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DATA
  include &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    phase: TRAIN
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 transform_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    crop_size: 224
    mean_file: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/mean.binaryproto&quot;&lt;/span&gt;
    mirror: &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 data_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/train_lmdb&quot;&lt;/span&gt;
    batch_size: 64
    backend: LMDB
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: DATA
  include &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    phase: TEST
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 transform_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    crop_size: 224
    mean_file: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/mean.binaryproto&quot;&lt;/span&gt;
    mirror: &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
 &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
 data_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;/raid/yuanyong/neuralcode/lmdb/val_lmdb&quot;&lt;/span&gt;
    batch_size: 52
    backend: LMDB
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

...
...
...

layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: INNER_PRODUCT
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc7&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  blobs_lr: 10          &lt;span class=&quot;c&quot;&gt;# 学习率是前面网络层是10倍&lt;/span&gt;
  blobs_lr: 20          &lt;span class=&quot;c&quot;&gt;# 学习率是前面网络层是10倍&lt;/span&gt;
  weight_decay: 1
  weight_decay: 0
  inner_product_param &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    num_output: 680     &lt;span class=&quot;c&quot;&gt;# 共680类&lt;/span&gt;
    weight_filler &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;gaussian&quot;&lt;/span&gt;
      std: 0.01
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
    bias_filler &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;constant&quot;&lt;/span&gt;
      value: 0
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;loss&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: SOFTMAX_LOSS
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;loss/loss&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
layers &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  name: &lt;span class=&quot;s2&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: ACCURACY
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;fc8_magic&quot;&lt;/span&gt;   &lt;span class=&quot;c&quot;&gt;# 改名字&lt;/span&gt;
  bottom: &lt;span class=&quot;s2&quot;&gt;&quot;label&quot;&lt;/span&gt;
  top: &lt;span class=&quot;s2&quot;&gt;&quot;accuracy&quot;&lt;/span&gt;
  include: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt; phase: TEST &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面配置测试输入的时候，&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;设置的是52，这个设置非常重要，我们一定要保证这个设置的&lt;code class=&quot;highlighter-rouge&quot;&gt;batch_size&lt;/code&gt;跟solver.prototxt里面设置的&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;乘起来等于测试样本数目。下面再看看solver.prototxt这个文件：&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;net: &lt;span class=&quot;s2&quot;&gt;&quot;train_val.prototxt&quot;&lt;/span&gt;
test_iter: 4005
test_interval: 5000
base_lr: 0.001
lr_policy: &lt;span class=&quot;s2&quot;&gt;&quot;step&quot;&lt;/span&gt;
gamma: 0.1
stepsize: 20000
display: 1000
max_iter: 50000
momentum: 0.9
weight_decay: 0.0005
snapshot: 5000
snapshot_prefix: &lt;span class=&quot;s2&quot;&gt;&quot;../models/snapshots_&quot;&lt;/span&gt;
solver_mode: GPU
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;比较重要的5个需要调整参数分别是&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;test_interval&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;stepsize&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;momentum&lt;/code&gt;。&lt;code class=&quot;highlighter-rouge&quot;&gt;test_iter&lt;/code&gt;怎么设置上面已经介绍。&lt;code class=&quot;highlighter-rouge&quot;&gt;test_interval&lt;/code&gt;表示迭代多少次进行一次验证及测试，&lt;code class=&quot;highlighter-rouge&quot;&gt;base_lr&lt;/code&gt;表示基础学习率，一般要比正常训练时的学习率要小，&lt;code class=&quot;highlighter-rouge&quot;&gt;stepsize&lt;/code&gt;跟步长相关，可以简单的理解为步长的分母，&lt;code class=&quot;highlighter-rouge&quot;&gt;momentum&lt;/code&gt;按推荐设置为0.9就可以。&lt;/p&gt;

&lt;p&gt;设置完上面的train_val.prototxt和solver.prototxt后，便可以开始正式fine-tuning了。&lt;/p&gt;

&lt;h3 id=&quot;正式fine-tuning&quot;&gt;正式fine-tuning&lt;/h3&gt;

&lt;p&gt;借助Caffe工具集下的caffe，我们只需要简单的执行下面命令即可完成网络的fine-tuning:&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$CAFFE_ROOT&lt;/span&gt;/build/tools/caffe train -solver  solver.prototxt -weights http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel -gpu 0,1 | tee log.txt
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;-gpu&lt;/code&gt;后面接的数字表示GPU设备的编号，这里我们使用了0卡和1卡，同时我们将训练的日志输出到log.txt里面。&lt;/p&gt;

&lt;h3 id=&quot;测试&quot;&gt;测试&lt;/h3&gt;

&lt;p&gt;完成了在Neural Codes的landmark数据集上的fine-tuning后，我们使用经过了fine-tuning后的模型在Oxford Building数据集上mAP提升了多少。为了方便对比，我们仍然提取fc6的特征，下面是不做ft(fine-tuning)和做ft的结果对比：&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;layer&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;mAP(128维)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fc6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;45.9%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;fc6+ft&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;60.2%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;可以看到，经过fine-tuning，mAP有了较大幅度的提升。从而也从实验的角度验证了对于检索任务，在数据允许的条件，对预训练模型进行fine-tuning显得非常的有必要。&lt;/p&gt;

&lt;h3 id=&quot;复现本文实验&quot;&gt;复现本文实验&lt;/h3&gt;

&lt;p&gt;如想复现本文实验，可以在这里&lt;a href=&quot;https://github.com/willard-yuan/cnn-cbir-benchmark/tree/master/fc_retrieval&quot;&gt;fc_retrieval&lt;/a&gt;找到相应的代码。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;p&gt;在本篇博文中，小白菜就5个方面的问题展开了总结和整理，分别是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CNN网络中哪一层最适合于做图像检索&lt;/li&gt;
  &lt;li&gt;基于pre-trained模型做图像检索几种典型的特征表示方法&lt;/li&gt;
  &lt;li&gt;抽取网络任意层的特征&lt;/li&gt;
  &lt;li&gt;数据增强(Data Augmentation)&lt;/li&gt;
  &lt;li&gt;VGGNet16网络模型fine-tuning实践&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;整个文章的基本组织结构依照典型的工科思维方式进行串接，即从理论到实践。&lt;/p&gt;
</description>
        <pubDate>Tue, 30 May 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/layer-selection-and-finetune-for-cbir.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/layer-selection-and-finetune-for-cbir.html</guid>
      </item>
    
      <item>
        <title>图像检索：再叙ANN Search</title>
        <description>&lt;p&gt;每逢碰到这个ANN的简称，小白菜总是想到Artificial Neural Network人工神经网络，不过这里要展开的ANN并不是Artificial Neural Network，而是已被小白菜之前写过很多次的Approximate Nearest Neighbor搜索。虽然读书的那会儿，这一块的工作专注得比较多，比如哈希，也整理过一个像模像样的工具包&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;hashing-baseline-for-image-retrieval&lt;/a&gt;，以及包括KD树、PQ乘积量化等近似最近邻搜索，但这些东西放在今天小白菜的知识体系里来看，依然自以为还非常的散乱。所以借再次有专研的机会之际，再做一次整理，完善自己在索引这方面的知识体系。&lt;/p&gt;

&lt;p&gt;在具体到不同类的索引方法分类前，小白菜以为，从宏观上对ANN有下面的认知显得很有必要：&lt;strong&gt;brute-force搜索的方式是在全空间进行搜索，为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历&lt;/strong&gt;。可以看到，正是因为缩减了遍历的空间大小范围，从而使得ANN能够处理大规模数据的索引。&lt;/p&gt;

&lt;p&gt;根据小白菜现有的对ANN的掌握，可以将ANN的方法分为三大类：基于树的方法、哈希方法、矢量量化方法。这三种方法里面，着重总结典型方法，其中由以哈希方法、矢量量化方法为主。&lt;/p&gt;

&lt;h2 id=&quot;基于树的方法&quot;&gt;基于树的方法&lt;/h2&gt;

&lt;p&gt;几乎所有的ANN方法都是对全空间的划分，所以基于树的方法也不例外。基于树的方法采用&lt;strong&gt;树&lt;/strong&gt;这种数据结构的方法来表达对全空间的划分，其中又以KD树最为经典，下面分别对KD树和&lt;a href=&quot;https://github.com/spotify/annoy&quot;&gt;Annoy&lt;/a&gt;进行简单介绍。&lt;/p&gt;

&lt;h3 id=&quot;kd数&quot;&gt;KD数&lt;/h3&gt;

&lt;p&gt;下面左图是KD树对全空间的划分过程，以及用树这种数据结构来表达的一个过程。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/kdTree_zpshq4ywnby.PNG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对KD树选择从哪一维度进行开始划分的标准，采用的是求每一个维度的方差，然后选择方差最大的那个维度开始划分。这里有一个比较有意思的问题是：&lt;strong&gt;为何要选择方差作为维度划分选取的标准&lt;/strong&gt;？我们都知道，方差的大小可以反映数据的波动性。方差大表示数据波动性越大，选择方差最大的开始划分空间，可以使得所需的划分面数目最小，反映到树数据结构上，可以使得我们构建的KD树的树深度尽可能的小。为了更进一步加深对这一点的认识，可以以一个简单的示例图说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/kd_zpslmugktds.JPEG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;假设不以方差最大的x轴为划分面(x_var = 16.25)，而是以y轴(y_var = 0.0)轴为划分面，如图中虚线所示，可以看到，该划分使得图中的四个点都落入在同一个子空间中，从而使得该划分成为一个无效的划分，体现在以树结构上，就是多了一层无用的树深度。而以x轴为初始划分则不同(图像实线所示)，以x轴为初始划分可以得到数据能够比较均匀的散布在左右两个子空间中，从而使得整体的查找时间能够最短。注意，在实际的kd树划分的时候，并不是图中虚线所示，而是选取中值最近的点。上面示意图构建的具体kd树如下所示：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kdtree&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;visualize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

                       &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

             &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;              &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;一般而言，在空间维度比较低的时候，KD树是比较高效的，当空间维度较高时，可以采用下面的哈希方法或者矢量量化方法。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;kd-trees are not suitable for efficiently finding the nearest neighbour in high dimensional spaces.&lt;br /&gt;
In very high dimensional spaces, the curse of dimensionality causes the algorithm to need to visit many more branches than in lower dimensional spaces. In particular, when the number of points is only slightly higher than the number of dimensions, the algorithm is only slightly better than a linear search of all of the points.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;annoy&quot;&gt;Annoy&lt;/h3&gt;

&lt;p&gt;Annoy是&lt;a href=&quot;https://github.com/erikbern&quot;&gt;Erik Bernhardsson&lt;/a&gt;写的一个以树为数据结构的近似最近邻搜索库，并用在&lt;a href=&quot;http://www.spotify.com/&quot;&gt;Spotify&lt;/a&gt;的推荐系统中。Annoy的核心是不断用选取的两个质心的法平面对空间进行分割，最终将每一个区分的子空间里面的样本数据限制在K以内。对于待插入的样本$x_i$，从根节点依次使用法向量跟$x_i$做内积运算，从而判断使用法平面的哪一边（左子树or右子树）。对于查询向量$q_i$，采用同样的方式（在树结构上体现为从根节点向叶子节点递归遍历），即可定位到跟$q_i$在同一个子空间或者邻近的子空间的样本，这些样本即为$q_i$近邻。&lt;/p&gt;

&lt;p&gt;为了提高查询的召回，Annoy采用建立多棵树的方式，这种做法是一种非常常见的做法，比如NV-tree也采用这种方式，哈希方法采用多表的哈希方法。&lt;/p&gt;

&lt;p&gt;值得注意的是，Annoy如果不保存原始特征，则Annoy只能返回查询的k个近邻，至于这k个里面的排序顺序是怎么样的，它是不知道的，如果需要知道，需要对这k个返回的结果，获取原始特征，再计算各自的相似度，排序一下即可得到这k个结果的排序。&lt;/p&gt;

&lt;p&gt;根据Annoy的定义的节点数据结构，如下：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ANNOY_NODE_ATTRIBUTE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Node&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_descendants&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;union&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;S&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;children&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Will possibly store more than 2
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dot_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// We let this one overflow intentionally. Need to allocate at least 1 to make GCC happy
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;其中&lt;code class=&quot;highlighter-rouge&quot;&gt;T v[1]&lt;/code&gt;保存原始特征，保存原始的特征的坏处是造成占用内存过大，索引文件过大。&lt;/p&gt;

&lt;h2 id=&quot;哈希方法&quot;&gt;哈希方法&lt;/h2&gt;

&lt;p&gt;哈希，顾名思义，就是将连续的实值散列化为0、1的离散值。在散列化的过程中，对散列化函数(也就是哈希函数)有一定的要求。根据学习的策略，可以将哈希方法分为无监督、有监督和半监督三种类型。在评估某种哈希方法用于图像检索的检索精度时，可以使用knn得到的近邻作为ground truth，也可以使用样本自身的类别作为ground truth。所以在实际评估准确度的时候，根据ground truth的定义，这里面是有一点小的trick的。通常对于无监督的哈希图像检索方法，由于我们使用的都是未标记的数据样本，所以我们会很自然的采用knn得到的近邻作为ground truth，但是对于图像检索的这一任务时，在对哈希函数的构造过程中，通常会有“相似的样本经编码后距离尽可能的近，不相似的样本编码后则尽可能的远”这一基本要求，这里讲到的相似，指语义的相似，因而你会发现，编码的基本要求放在无监督哈希方法里，似乎与采用knn得到的近邻作为ground truth的评价方式有些南辕北辙。对无监督哈希方法的ground truth一点小的疑惑在小白菜读书的时候就心存这样的困惑，一直悬而未解。当然，在做无监督的图像哈希方法，采用样本自身的类别作为ground truth是毋庸置疑的。&lt;/p&gt;

&lt;p&gt;小白菜读书那会儿，研究了很多的哈希图像检索方法（见&lt;a href=&quot;https://github.com/willard-yuan/hashing-baseline-for-image-retrieval&quot;&gt;hashing-baseline-for-image-retrieval&lt;/a&gt;），有时候总会给一些工程实践上的错觉（在今天看来是这样的），即新论文里的方法远远碾压经典的方法，那是不是在实际中这些方法就很work很好使。实践的经历告诉小白菜，还是经典的东西更靠谱，不是因为新的方法不好，而是新的事物需要经过时间的沉淀与优化。&lt;/p&gt;

&lt;p&gt;所以，这里不会对近两年的哈希方法做铺陈，而是聊一聊工程中在要使用到哈希方法的场景下一般都会选用的局部敏感哈希（Local Sensitive Hashing, LSH）。&lt;/p&gt;

&lt;h3 id=&quot;local-sensitive-hashing&quot;&gt;Local Sensitive Hashing&lt;/h3&gt;

&lt;p&gt;关于LSH的介绍，小白菜以为，&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/wiki/LSH-Primer&quot;&gt;Locality-Sensitive Hashing: a Primer&lt;/a&gt;这个讲解得极好，推荐一读。下面是小白菜结合自己的理解，提炼的一些在小白菜看来需要重点理解的知识（附上LSH划分空间示意图，在进行理解的时候可以参照改图）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/lsh_ex_zps0lryoykz.PNG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;局部敏感是啥&quot;&gt;局部敏感是啥？&lt;/h4&gt;

&lt;p&gt;当一个函数（或者更准确的说，哈希函数家族）具有如下属性的时候，我们说该哈希函数是局部敏感的：相近的样本点对比相远的样本点对更容易发生碰撞。&lt;/p&gt;

&lt;h4 id=&quot;用哈希为什么可以加速查找&quot;&gt;用哈希为什么可以加速查找？&lt;/h4&gt;

&lt;p&gt;对于brute force搜索，需要遍历数据集中的所有点，而使用哈希，我们首先找到查询样本落入在哪个cell(即所谓的桶)中，如果空间的划分是在我们想要的相似性度量下进行分割的，则查询样本的最近邻将极有可能落在查询样本的cell中，如此我们只需要在当前的cell中遍历比较，而不用在所有的数据集中进行遍历。&lt;/p&gt;

&lt;h4 id=&quot;为什么要用多表哈希&quot;&gt;为什么要用多表哈希？&lt;/h4&gt;

&lt;p&gt;对于单表哈希，当我们的哈希函数数目K取得太大，查询样本与其对应的最近邻落入同一个桶中的可能性会变得很微弱，针对这个问题，我们可以重复这个过程L次，从而增加最近邻的召回率。这个重复L次的过程，可以转化为构建L个哈希表，这样在给定查询样本时，我们可以找到L个哈希桶（每个表找到一个哈希桶），然后我们在这L个哈希表中进行遍历。这个过程相当于构建了K*L个哈希函数(注意是“相当”，不要做“等价”理解)。&lt;/p&gt;

&lt;h4 id=&quot;多表哈希中哈希函数数目k和哈希表数目l如何选取&quot;&gt;多表哈希中哈希函数数目K和哈希表数目L如何选取？&lt;/h4&gt;

&lt;p&gt;哈希函数数目K如果设置得过小，会导致每一个哈希桶中容纳了太多的数据点，从而增加了查询响应的时间；而当K设置得过大时，会使得落入每个哈希桶中的数据点变小，而为了增加召回率，我们需要增加L以便构建更多的哈希表，但是哈希表数目的增加会导致更多的内存消耗，并且也使得我们需要计算更多的哈希函数，同样会增加查询相应时间。这听起来非常的不妙，但是在K过大或过小之间仍然可以找到一个比较合理的折中位置。通过选取合理的K和L，我们可以获得比线性扫描极大的性能提升。&lt;/p&gt;

&lt;h4 id=&quot;multiprobe-lsh是为了解决什么问题&quot;&gt;Multiprobe LSH是为了解决什么问题？&lt;/h4&gt;

&lt;p&gt;多probe LSH主要是为了提高查找准确率而引入的一种策略。首先解释一下什么是Multiprobe。对于构建的L个哈希表，我们在每一个哈希表中找到查询样本落入的哈希桶，然后再在这个哈希桶中做遍历，而Multiprobe指的是我们不止在查询样本所在的哈希桶中遍历，还会找到其他的一些哈希桶，然后这些找到的T个哈希桶中进行遍历。这些其他哈希桶的选取准则是：跟查询样本所在的哈希桶邻近的哈希桶，“邻近”指的是汉明距离度量下的邻近。&lt;/p&gt;

&lt;p&gt;通常，如果不使用Multiprobe，我们需要的哈希表数目L在100到1000之间，在处理大数据集的时候，其空间的消耗会非常的高，幸运地是，因为有了上面的Multiprobe的策略，LSH在任意一个哈希表中查找到最近邻的概率变得更高，从而使得我们能到减少哈希表的构建数目。&lt;/p&gt;

&lt;p&gt;综上，对于LSH，涉及到的主要的参数有三个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K，每一个哈希表的哈希函数（空间划分）数目&lt;/li&gt;
  &lt;li&gt;L，哈希表（每一个哈希表有K个哈希函数）的数目&lt;/li&gt;
  &lt;li&gt;T，近邻哈希桶的数目，即the number of probes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这三个设置参数可以按照如下顺序进行：首先，根据可使用的内存大小选取L，然后在K和T之间做出折中：哈希函数数目K越大，相应地，近邻哈希桶的数目的数目T也应该设置得比较大，反之K越小，L也可以相应的减小。获取K和L最优值的方式可以按照如下方式进行：对于每个固定的K，如果在查询样本集上获得了我们想要的精度，则此时T的值即为合理的值。在对T进行调参的时候，我们不需要重新构建哈希表，甚至我们还可以采用二分搜索的方式来加快T参数的选取过程。&lt;/p&gt;

&lt;h3 id=&quot;lsh开源工具包&quot;&gt;LSH开源工具包&lt;/h3&gt;

&lt;p&gt;关于LSH开源工具库，有很多，这里推荐两个LSH开源工具包：&lt;a href=&quot;https://github.com/kayzhu/LSHash&quot;&gt;LSHash&lt;/a&gt;和&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;, 分别对应于学习和应用场景。&lt;/p&gt;

&lt;h4 id=&quot;lshash&quot;&gt;LSHash&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/kayzhu/LSHash&quot;&gt;LSHash&lt;/a&gt;非常适合用来学习，里面实现的是最经典的LSH方法，并且还是单表哈希。哈希函数的系数采用随机的方式生成，具体代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_generate_uniform_planes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot; Generate uniformly distributed hyperplanes and return it as a 2D
    numpy array.
    &quot;&quot;&quot;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hash_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;hash_size&lt;/code&gt;为哈希函数的数目，即前面介绍的K。整个框架，不论是LSH的哈希函数的生成方式，还是LSH做查询，都极其的中规中矩，所以用来作为了解LSH的过程，再适合不过。如果要在实用中使用LSH，可以使用&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;。&lt;/p&gt;

&lt;h4 id=&quot;falconn&quot;&gt;FALCONN&lt;/h4&gt;

&lt;p&gt;&lt;a href=&quot;https://falconn-lib.org/&quot;&gt;FALCONN&lt;/a&gt;是经过了极致优化的LSH，其对应的论文为NIPS 2015 &lt;a href=&quot;https://people.csail.mit.edu/ludwigs/papers/nips15_crosspolytopelsh.pdf&quot;&gt;Practical and Optimal LSH for Angular Distance&lt;/a&gt;，&lt;a href=&quot;https://people.csail.mit.edu/indyk/&quot;&gt;Piotr Indyk&lt;/a&gt;系作者之一（Piotr Indyk不知道是谁？&lt;a href=&quot;http://web.mit.edu/andoni/www/LSH/index.html&quot;&gt;E2LSH&lt;/a&gt;这个页面对于看过LSH的应该非常眼熟吧），论文有些晦涩难懂，不过FALCONN工具包却是极其容易使用的，提供有C++使用的例子&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/benchmark/random_benchmark.cc&quot;&gt;random_benchmark.cc&lt;/a&gt;以及Python的例子&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/python/benchmark/random_benchmark.py&quot;&gt;random_benchmark.py&lt;/a&gt;，另外文档非常的详实，具体可参阅&lt;a href=&quot;https://falconn-lib.org/docs/namespacefalconn.html&quot;&gt;falconn Namespace Reference&lt;/a&gt;和&lt;a href=&quot;https://falconn-lib.org/pdoc/falconn/&quot;&gt;falconn module&lt;/a&gt;。下面将其Python例子和C++例子中初始化索引以及构建哈希表的部分提取出来，对其中的参数做一下简要的分析。&lt;/p&gt;

&lt;p&gt;Python初始化与构建索引&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/python/benchmark/random_benchmark.py#L127&quot;&gt;L127&lt;/a&gt;：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Hyperplane hashing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;falconn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHConstructionParameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lsh_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'hyperplane'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'negative_inner_product'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'flat_hash_table'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;833840234&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Hyperplane hash&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;timeit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;default_timer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;falconn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHIndex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hp_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_num_probes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2464&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;C++初始化与构建索引&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/blob/master/src/benchmark/random_benchmark.cc#L194&quot;&gt;L194&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;language-c++ highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// Hyperplane hashing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHConstructionParameters&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dimension&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lsh_family&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LSHFamily&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hyperplane&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;storage_hash_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_tables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_setup_threads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;^&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;833840234&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Hyperplane hash&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;endl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Timer&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hp_construction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;unique_ptr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSHNearestNeighborTable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hptable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;move&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;construct_table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Vec&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_hp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hptable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_num_probes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2464&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;可以看到，有3个很重要的参数，分别是&lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;l&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;set_num_probes&lt;/code&gt;，对应的具体意义前面已经解释，这里不再赘述。&lt;/p&gt;

&lt;p&gt;FALCONN的索引构建过程非常快，百万量级的数据，维度如果是128维，其构建索引时间大概2-3min的样子，实时搜索可以做到几毫秒的响应时间。总之，这是小白菜见过的构建索引时间最短查询响应时间也极快的ANN工具库。&lt;/p&gt;

&lt;p&gt;另外谈一下数据规模问题。对于小数据集和中型规模的数据集(几个million-几十个million)， FALCONN和&lt;a href=&quot;https://github.com/searchivarius/nmslib&quot;&gt;NMSLIB&lt;/a&gt;是一个非常不错的选择，如果对于大型规模数据集(几百个million以上)，基于矢量量化的&lt;a href=&quot;https://github.com/facebookresearch/faiss/&quot;&gt;Faiss&lt;/a&gt;是一个明智的选择。关于这方面的讨论，可以参阅小白菜参阅的讨论&lt;a href=&quot;https://github.com/facebookresearch/faiss/issues/23&quot;&gt;benchmark&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;当然，FALCONN还不是很完善，比如对于数据的动态增删目前还不支持，具体的讨论可以参见&lt;a href=&quot;https://github.com/FALCONN-LIB/FALCONN/issues/2&quot;&gt;Add a dynamic LSH table&lt;/a&gt;。其实这不是FALCONN独有的问题，NMSLIB目前也不支持。一般而言，动态的增删在实际应用场合是一个基本的要求，但是我们应注意到，增删并不是毫无限制的，在增删频繁且持续了一段时间后，这是的数据分布已经不是我们原来建索引的数据分布形式了，我们应该重新构建索引。在这一点上，Faiss支持数据的动态增删。&lt;/p&gt;

&lt;p&gt;对于哈希方法及其典型代表局部敏感哈希，暂时就整理到这里了。下面小白菜对基于矢量量化的方法谈一谈自己理解。&lt;/p&gt;

&lt;h2 id=&quot;矢量量化方法&quot;&gt;矢量量化方法&lt;/h2&gt;

&lt;p&gt;矢量量化方法，即vector quantization，其具体定义为：&lt;a href=&quot;http://blog.pluskid.org/?p=57&quot;&gt;将一个向量空间中的点用其中的一个有限子集来进行编码的过程&lt;/a&gt;。在矢量量化编码中，&lt;a href=&quot;http://baike.baidu.com/item/%E7%9F%A2%E9%87%8F%E9%87%8F%E5%8C%96&quot;&gt;关键是码本的建立和码字搜索算法&lt;/a&gt;。比如常见的聚类算法，就是一种矢量量化方法。而在ANN近似最近邻搜索中，向量量化方法又以乘积量化(PQ, Product Quantization)最为典型。在之前的博文&lt;a href=&quot;http://yongyuan.name/blog/cbir-technique-summary.html&quot;&gt;基于内容的图像检索技术&lt;/a&gt;的最后，对PQ乘积量化的方法做过简单的概要。在这一小节里，小白菜结合自己阅读的论文和代码对PQ乘积量化、倒排乘积量化(IVFPQ)做一种更加直观的解释。&lt;/p&gt;

&lt;h3 id=&quot;pq乘积量化&quot;&gt;PQ乘积量化&lt;/h3&gt;

&lt;p&gt;PQ乘积量化的核心思想还是聚类，或者说具体应用到ANN近似最近邻搜索上，K-Means是PQ乘积量化子空间数目为1的特例。PQ乘积量化生成码本和量化的过程可以用如下图示来说明：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_zpsybhprown.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在训练阶段，针对N个训练样本，假设样本维度为128维，我们将其切分为4个子空间，则每一个子空间的维度为32维，然后我们在每一个子空间中，对子向量采用K-Means对其进行聚类(图中示意聚成256类)，这样每一个子空间都能得到一个码本。这样训练样本的每个子段，都可以用子空间的聚类中心来近似，对应的编码即为类中心的ID。如图所示，通过这样一种编码方式，训练样本仅使用的很短的一个编码得以表示，从而达到量化的目的。对于待编码的样本，将它进行相同的切分，然后在各个子空间里逐一找到距离它们最近的类中心，然后用类中心的id来表示它们，即完成了待编码样本的编码。&lt;/p&gt;

&lt;p&gt;正如前面所说的，在矢量量化编码中，关键是码本的建立和码字的搜索算法，在上面，我们得到了建立的码本以及量化编码的方式。剩下的重点就是查询样本与dataset中的样本距离如何计算的问题了。&lt;/p&gt;

&lt;p&gt;在查询阶段，PQ同样在计算查询样本与dataset中各个样本的距离，只不过这种距离的计算转化为间接近似的方法而获得。PQ乘积量化方法在计算距离的时候，有两种距离计算方式，一种是对称距离，另外一种是非对称距离。非对称距离的损失小(也就是更接近真实距离)，实际中也经常采用这种距离计算方式。下面过程示意的是查询样本来到时，以非对称距离的方式(红框标识出来的部分)计算到dataset样本间的计算示意：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/PQ_search_zpskgugtocx.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;具体地，查询向量来到时，按训练样本生成码本的过程，将其同样分成相同的子段，然后在每个子空间中，计算子段到该子空间中所有聚类中心得距离，如图中所示，可以得到4*256个距离，这里为便于后面的理解说明，小白菜就把这些算好的距离称作距离池。在计算库中某个样本到查询向量的距离时，比如编码为(124, 56, 132, 222)这个样本到查询向量的距离时，我们分别到距离池中取各个子段对应的距离即可，比如编码为124这个子段，在第1个算出的256个距离里面把编号为124的那个距离取出来就可，所有子段对应的距离取出来后，将这些子段的距离求和相加，即得到该样本到查询样本间的非对称距离。所有距离算好后，排序后即得到我们最终想要的结果。&lt;/p&gt;

&lt;p&gt;从上面这个过程可以很清楚地看出PQ乘积量化能够加速索引的原理：即将全样本的距离计算，转化为到子空间类中心的距离计算。比如上面所举的例子，原本brute-force search的方式计算距离的次数随样本数目N成线性增长，但是经过PQ编码后，对于耗时的距离计算，只要计算4*256次，几乎可以忽略此时间的消耗。另外，从上图也可以看出，对特征进行编码后，可以用一个相对比较短的编码来表示样本，自然对于内存的消耗要大大小于brute-force search的方式。&lt;/p&gt;

&lt;p&gt;在某些特殊的场合，我们总是希望获得精确的距离，而不是近似的距离，并且我们总是喜欢获取向量间的余弦相似度（余弦相似度距离范围在[-1,1]之间，便于设置固定的阈值），针对这种场景，可以针对PQ乘积量化得到的前top@K做一个brute-force search的排序。&lt;/p&gt;

&lt;h3 id=&quot;倒排乘积量化&quot;&gt;倒排乘积量化&lt;/h3&gt;

&lt;p&gt;倒排PQ乘积量化(IVFPQ)是PQ乘积量化的更进一步加速版。其加速的本质逃不开小白菜在最前面强调的是加速原理：&lt;strong&gt;brute-force搜索的方式是在全空间进行搜索，为了加快查找的速度，几乎所有的ANN方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历&lt;/strong&gt;。在上一小节可以看出，PQ乘积量化计算距离的时候，距离虽然已经预先算好了，但是对于每个样本到查询样本的距离，还是得老老实实挨个去求和相加计算距离。但是，实际上我们感兴趣的是那些跟查询样本相近的样本（小白菜称这样的区域为感兴趣区域），也就是说老老实实挨个相加其实做了很多的无用功，如果能够通过某种手段快速将全局遍历锁定为感兴趣区域，则可以舍去不必要的全局计算以及排序。倒排PQ乘积量化的”倒排“，正是这样一种思想的体现，在具体实施手段上，采用的是通过聚类的方式实现感兴趣区域的快速定位，在倒排PQ乘积量化中，聚类可以说应用得淋漓尽致。&lt;/p&gt;

&lt;p&gt;倒排PQ乘积量化整个过程如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0408/IVFPQ_zpswvosr8a6.PNG&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在PQ乘积量化之前，增加了一个粗量化过程。具体地，先对N个训练样本采用K-Means进行聚类，这里聚类的数目一般设置得不应过大，一般设置为1024差不多，这种可以以比较快的速度完成聚类过程。得到了聚类中心后，针对每一个样本x_i，找到其距离最近的类中心c_i后，两者相减得到样本x_i的残差向量(x_i-c_i)，后面剩下的过程，就是针对(x_i-c_i)的PQ乘积量化过程，此过程不再赘述。&lt;/p&gt;

&lt;p&gt;在查询的时候，通过相同的粗量化，可以快速定位到查询向量属于哪个c_i（即在哪一个感兴趣区域），然后在该感兴趣区域按上面所述的PQ乘积量化距离计算方式计算距离。&lt;/p&gt;
</description>
        <pubDate>Sat, 08 Apr 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/ann-search.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/ann-search.html</guid>
      </item>
    
      <item>
        <title>图像检索：拓展查询(Query Expansion)</title>
        <description>&lt;p&gt;拓展查询(QE, Query Expansion): 指对返回的前top@K个结果，包括查询样本本身，对它们的特征求和取平均，再做一次查询，此过程称为拓展查询。&lt;/p&gt;

&lt;p&gt;从上面的定义可以看出，拓展查询属于重排的一种方式。通过Query Expansion，以达到提高检索召回率的目的。前面的博文&lt;a href=&quot;http://yongyuan.name/blog/fitting-line-with-ransac.html&quot;&gt;RANSAC算法做直线拟合&lt;/a&gt;曾介绍过RANSAC的基本思想，放在词袋模型里（相应博文见&lt;a href=&quot;http://yongyuan.name/blog/CBIR-BoW-for-image-retrieval-and-practice.html&quot;&gt;图像检索：BoW图像检索原理与实战&lt;/a&gt;），我们可以使用RANSAC方法或Weak Geometry Consistency方法做几何校正，进行重排以提高检索的精度。在这篇博文中，小白菜暂时抛开其他的重排方法，重点分析Query Expansion对图像检索精度的提升。&lt;/p&gt;

&lt;p&gt;根据小白菜读图像检索论文获得的对Query Expansion的感知，做完Query Expansion能够获得百分之几的精度提升。为了证实Query Expansion对检索精度的改善，在过去一段时间里，小白菜在Oxford Building数据库上对其做了验证。下面是小白菜对Query Expansion的实验整理和总结。&lt;/p&gt;

&lt;h2 id=&quot;特征表达&quot;&gt;特征表达&lt;/h2&gt;

&lt;p&gt;Oxford Building图像数据库，每一幅图像提取的是一个512维的CNN特征，即对于Oxford Building图像数据库，我们得到5064个512维的特征。&lt;/p&gt;

&lt;h2 id=&quot;特征索引&quot;&gt;特征索引&lt;/h2&gt;

&lt;p&gt;直接采用brute线性扫描，因为图库才5064张图像，所以没必要建索引。在实际应用中，我们可以采用哈希、倒排PQ等方式，这一部分可以细讲很多，有机会的话，小白菜单独拿一个篇幅整理实用的索引方法。&lt;/p&gt;

&lt;h2 id=&quot;评价指标&quot;&gt;评价指标&lt;/h2&gt;

&lt;p&gt;实验评价指标采用平均检索精度(mAP, mean average precision), mAP如何计算可以阅读&lt;a href=&quot;http://yongyuan.name/blog/evaluation-of-information-retrieval.html&quot;&gt;信息检索评价指标&lt;/a&gt;，里面有对mAP如何计算的详细介绍。&lt;/p&gt;

&lt;p&gt;对于Oxford Building图像数据库，mAP的计算过程有必要详细介绍一下。Oxford Building的groundtruth有三类：good, ok和junk。对于某个检索结果，如果它在good和ok中，则被判为是与查询图像相关的；如果在junk中，则被判为是不相关的。我们可以细致的阅读一下Oxford Building的mAP计算代码：&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute_ap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;amb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ranked_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_recall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_precision&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  
  &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intersect_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;kt&quot;&gt;size_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranked_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranked_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ranked_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intersect_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intersect_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;intersect_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;old_recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_precision&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;old_recall&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;old_precision&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;其中，&lt;code class=&quot;highlighter-rouge&quot;&gt;pos&lt;/code&gt;即是由good和ok构成的set，&lt;code class=&quot;highlighter-rouge&quot;&gt;amb&lt;/code&gt;是junk构成的set，&lt;code class=&quot;highlighter-rouge&quot;&gt;ranked_list&lt;/code&gt;即查询得到的结果。可以看到Oxford Building上计算的AP是检索准确率(precision)和检索召回率(recall)曲线围成的面积(梯形面积，积分思想)，mAP即是对AP的平均。&lt;/p&gt;

&lt;p&gt;理解完了Oxford Building的mAP计算过程，还有一个需要考虑的问题是：对于查询图像特征的提取，我们要不要把Oxford Building提供的区域框用上，即在提取特征的时候，我们是在整个图像提取特征，还是在区域框内提取特征？&lt;strong&gt;在图像检索的论文中，在计算Oxford Building的mAP时，都是在区域框内提取特征。但是放在实际中，我们肯定是希望我们的图像检索方法能够尽可能的减少交互，即在不框选区域的时候，也能够取得很好的检索精度&lt;/strong&gt;。所以，基于这样的意图，在实际中测评检索算法的mAP时，小白菜更喜欢采用在整个图像上提取特征的方式。当然，如果不嫌麻烦的话，可以两种方式都测评一下。&lt;/p&gt;

&lt;h2 id=&quot;查询拓展对map的提升&quot;&gt;查询拓展对mAP的提升&lt;/h2&gt;

&lt;p&gt;库内查询，所以返回的top@1为查询图像自身，并且采用的是全图查询(即上面提到的对于查询图像是在整个图像上提取特征，而不是在区域框内提取特征)，表中top@K表示取前K个样本求和取平均。&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;top@K&lt;/th&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;th&gt;10&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MAP&lt;/td&gt;
      &lt;td&gt;61.91%&lt;/td&gt;
      &lt;td&gt;61.91%&lt;/td&gt;
      &lt;td&gt;65.42%&lt;/td&gt;
      &lt;td&gt;66.52%&lt;/td&gt;
      &lt;td&gt;66.07%&lt;/td&gt;
      &lt;td&gt;66.38%&lt;/td&gt;
      &lt;td&gt;66.51%&lt;/td&gt;
      &lt;td&gt;65.65%&lt;/td&gt;
      &lt;td&gt;65.16%&lt;/td&gt;
      &lt;td&gt;63.46%&lt;/td&gt;
      &lt;td&gt;62.41%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;上面表格中mAP随top@K用曲线表示如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://ose5hybez.bkt.clouddn.com/2017/0205/qe_map_zpsbat8vy5x.PNG&quot; alt=&quot;drawing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在不做Query Expansion的时候，即top@K=0时，mAP为61.91%。因为查询属于库内查询，所以top@K=1时，仍然是查询向量本身，故结果与top@K=0是一样的。从实验的结果可以看出，Query Expansion确实能够提升检索的精度，在top@K=3的时候，取得了最高的检索精度。相比于不做Query Expansion，Query Expansion可以提高4%-5%的检索精度。&lt;/p&gt;

&lt;p&gt;所以，&lt;strong&gt;在实际中，做Query Expansion完全是有必要的，一则是它实现简单，二则是它提升的效果还是比较明显的&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 05 Feb 2017 00:00:00 +0800</pubDate>
        <link>http://localhost:4000//blog/cbir-query-expansion.html</link>
        <guid isPermaLink="true">http://localhost:4000//blog/cbir-query-expansion.html</guid>
      </item>
    
  </channel>
</rss>
